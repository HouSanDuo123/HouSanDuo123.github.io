<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>1</title>
      <link href="2021/03/03/1/"/>
      <url>2021/03/03/1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Cesium之Entities2</title>
      <link href="2020/12/21/Cesium%E4%B8%AD%E7%9A%84Entity2/"/>
      <url>2020/12/21/Cesium%E4%B8%AD%E7%9A%84Entity2/</url>
      
        <content type="html"><![CDATA[<h1 id="填充材料"><a href="#填充材料" class="headerlink" title="填充材料"></a>填充材料</h1><ul><li>填充包括四种，分别是图片、棋盘、网格、条纹<ul><li>图片：直接从源文件中加载；</li><li>棋盘、条纹：设置奇偶颜色分别是什么以及实体范围内有多少个棋盘或条纹，其中棋盘是一个二维数组，条纹是条纹的个数；</li><li>网格：直接设置网格的数量（二维）以及网格边框颜色及网格颜色；</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">var viewer &#x3D; new Cesium.Viewer(&#39;cesiumContainer&#39;);</span><br><span class="line"></span><br><span class="line">var entity &#x3D; viewer.entities.add(&#123;</span><br><span class="line">position : Cesium.Cartesian3.fromDegrees(-103.0, 40.0),</span><br><span class="line">ellipse : &#123;</span><br><span class="line">semiMinorAxis : 250000.0,</span><br><span class="line">semiMajorAxis : 400000.0,</span><br><span class="line">material : Cesium.Color.BLUE.withAlpha(0.5)</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var ellipse &#x3D; entity.ellipse;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;填充图片</span><br><span class="line">ellipse.material &#x3D; &#39;.&#x2F;Source&#x2F;14.jpg&#39;;</span><br></pre></td></tr></table></figure><img src="/2020/12/21/Cesium%E4%B8%AD%E7%9A%84Entity2/1.png" class><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;填充棋盘</span><br><span class="line">ellipse.material &#x3D; new Cesium.CheckerboardMaterialProperty(&#123;</span><br><span class="line">evencolor : Cesium.Color.WHITE,</span><br><span class="line">oddcolor : Cesium.Color.BLACK,</span><br><span class="line">repeat : new Cesium.Cartesian2(4, 4)&#x2F;&#x2F;二维里面重复4*4</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><img src="/2020/12/21/Cesium%E4%B8%AD%E7%9A%84Entity2/2.png" class><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;填充条纹</span><br><span class="line">ellipse.material &#x3D; new Cesium.StripeMaterialProperty(&#123;</span><br><span class="line">evenColor : Cesium.Color.WHITE,</span><br><span class="line">oddColor : Cesium.color.BLACK,</span><br><span class="line">repeat : 32,&#x2F;&#x2F;一维里面重复32个</span><br><span class="line">&#x2F;&#x2F;offset : 3,</span><br><span class="line">&#x2F;&#x2F;orientation : VERTICAL</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><img src="/2020/12/21/Cesium%E4%B8%AD%E7%9A%84Entity2/3.png" class><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;填充网格</span><br><span class="line">ellipse.material &#x3D; new Cesium.GridMaterialProperty(&#123;</span><br><span class="line">color : Cesium.Color.YELLOW,</span><br><span class="line">cellAlpha : 0.2,</span><br><span class="line">lineCount : new Cesium.Cartesian2(8, 8),</span><br><span class="line">lineThickness : new Cesium.Cartesian2(2.0, 2.0)</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><img src="/2020/12/21/Cesium%E4%B8%AD%E7%9A%84Entity2/4.png" class><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">viewer.zoomTo(viewer.entities);</span><br></pre></td></tr></table></figure><h1 id="相机控制"><a href="#相机控制" class="headerlink" title="相机控制"></a>相机控制</h1><ul><li><p>分为两种控制方式：zoomTo和flyTo</p></li><li><p>zoomTo(target, offset)</p><ul><li>tartget为目标值，offset为heading、pitch、range</li><li>异步设置摄像机以查看提供的一个或多个实体或数据源。如果数据源仍在加载过程中，或者可视化仍在加载中，则此方法在执行缩放之前等待数据准备就绪。</li><li>偏移是在以边界球的中心为中心的局部**<u>东-北-上</u><strong>参考系中的</strong>航向/俯仰/范围**。航向角和俯仰角是在局部的东北上参考系中定义的。航向是从y轴到x轴的角度。间距是从xy平面开始的旋转。正俯仰角在平面上方。负俯仰角在平面下方。范围是到中心的距离。如果范围为零，则将计算范围，以使整个边界球都可见。</li><li>在2D模式下，必须有一个俯视图。相机将被放置在俯视目标上方。目标上方的高度将是范围。航向将根据偏移量确定。如果无法从偏移量确定航向，则航向将为北。</li></ul></li><li><p>flyTo(target, option)</p><ul><li><p>tartget为目标值，offset为duration、maximumHeight、offset</p></li><li><p>duration为飞行持续时间；</p></li><li><p>maximumHeight为飞行最大高度</p></li><li><p>offset同上</p></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">var viewer &#x3D; new Cesium.Viewer(&#39;cesiumContainer&#39;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;添加一个多边形</span><br><span class="line">var wyoming &#x3D; viewer.entities.add(&#123;</span><br><span class="line">name : &#39;Wyoming&#39;,</span><br><span class="line">polygon : &#123;</span><br><span class="line">    hierarchy : Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">-109,45,</span><br><span class="line">-105,45,</span><br><span class="line">-104,44,</span><br><span class="line">-104,43,</span><br><span class="line">-104,41,</span><br><span class="line">-105,40,</span><br><span class="line">-107,41,</span><br><span class="line">-109,40,</span><br><span class="line">-111,40,</span><br><span class="line">-111,42,</span><br><span class="line">-111,44,</span><br><span class="line">-111,45    </span><br><span class="line">]),</span><br><span class="line">minimumPixelSize : 800,</span><br><span class="line">        maximumScale : 1000,</span><br><span class="line">height : 0,</span><br><span class="line">material : Cesium.Color.RED.withAlpha(0.5),</span><br><span class="line">outline : true,</span><br><span class="line">outlineColor : Cesium.Color.BLACK</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">var heading &#x3D; Cesium.Math.toRadians(90);</span><br><span class="line">var pitch &#x3D; Cesium.Math.toRadians(-30);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">viewer.zoomTo(wyoming, new Cesium.HeadingPitchRange(heading, pitch));</span><br><span class="line"></span><br><span class="line">viewer.flyTo(wyoming).then(function()&#123;</span><br><span class="line">if(result)&#123;</span><br><span class="line">viewer.selectedEntity &#x3D; wyoming;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Points，Billboards-and-labels"><a href="#Points，Billboards-and-labels" class="headerlink" title="Points，Billboards, and labels"></a>Points，Billboards, and labels</h1><ul><li>Points就是一个点，按照添加实体点来设置参数即可；</li><li>Billboards是一个标签，先确定点之后，然后在该点的位置加载一个标签图片即可；</li><li>在设置label的时候，text的相关参数类似于实体，位置可以是以点作为参考系；</li></ul><h2 id="兴趣点"><a href="#兴趣点" class="headerlink" title="兴趣点"></a>兴趣点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;Points，Billboards, and labels</span><br><span class="line">&#x2F;&#x2F;Cesium中用于表示兴趣点</span><br><span class="line"></span><br><span class="line">var viewer &#x3D; new Cesium.Viewer(&#39;cesiumContainer&#39;);</span><br><span class="line">var OlympicPark &#x3D; viewer.entities.add(&#123;</span><br><span class="line">name : &#39;Beijing Olympic Park&#39;,</span><br><span class="line">position : Cesium.Cartesian3.fromDegrees(116.396, 39.993),</span><br><span class="line">point : &#123;</span><br><span class="line">pixelSize : 5,</span><br><span class="line">color : Cesium.Color.RED,</span><br><span class="line">outlinColor : Cesium.Color.WHITE,</span><br><span class="line">outlineWidth : 2</span><br><span class="line">&#125;,</span><br><span class="line">label : &#123;</span><br><span class="line">text : &#39;Beijing Olympic Park&#39;,</span><br><span class="line">font : &#39;14pt monospace&#39;,</span><br><span class="line">style : Cesium.LabelStyle.FILL_AND_OUTLINE,</span><br><span class="line">outlineWidth : 2,</span><br><span class="line">verticalOrigin : Cesium.VerticalOrigin.BOTTOM,</span><br><span class="line">pixelOffset : new Cesium.Cartesian2(0, -9)</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">viewer.zoomTo(viewer.entities);</span><br></pre></td></tr></table></figure><h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">var OlympicPark &#x3D; viewer.entities.add(&#123;</span><br><span class="line">&#x2F;&#x2F;name : &#39;Beijing Olympic Park&#39;,</span><br><span class="line">position : Cesium.Cartesian3.fromDegrees(116.396, 39.993),</span><br><span class="line">billboard : &#123;</span><br><span class="line">image : &#39;https:&#x2F;&#x2F;store-images.s-microsoft.com&#x2F;image&#x2F;apps.19467.14562462990742545.f75ad677-b304-4b69-b837-c12e4f56eb06.3c421dde-9529-47fb-85f3-5c3ffb33bf77?w&#x3D;1399&amp;h&#x3D;787&amp;q&#x3D;90&amp;format&#x3D;jpg&#39;,</span><br><span class="line">width : 64, </span><br><span class="line">height : 64</span><br><span class="line">&#125;,</span><br><span class="line">label : &#123;</span><br><span class="line">&#x2F;&#x2F;text : &#39;Beijing Olympic Park&#39;,</span><br><span class="line">font : &#39;15pt monospace&#39;,</span><br><span class="line">style : Cesium.LabelStyle.FILL_AND_OUTLINE,</span><br><span class="line">outlineWidth : 1,</span><br><span class="line">verticalOrigin : Cesium.VerticalOrigin.BOTTOM,</span><br><span class="line">pixelOffset : new Cesium.Cartesian2(0, 100)</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Cesium </tag>
            
            <tag> Entity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cesium之Entities1</title>
      <link href="2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/"/>
      <url>2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/</url>
      
        <content type="html"><![CDATA[<h1 id="Cesium中的实体Entity"><a href="#Cesium中的实体Entity" class="headerlink" title="Cesium中的实体Entity"></a>Cesium中的实体Entity</h1><p>包括Boxes、</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;这两行函数针对每一个代码块的代码都需要加上才能运行成功；分别放置在首尾位置</span><br><span class="line"></span><br><span class="line">var viewer &#x3D; new Cesium.Viewer(&#39;cesiumContainer&#39;);</span><br><span class="line">&#x2F;&#x2F;viewer.entities可以替换为每个entity的名字，例如blueBox、redBox</span><br><span class="line">viewer.zoomTo(viewer.entities);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Boxes"><a href="#Boxes" class="headerlink" title="Boxes"></a>Boxes</h1><h2 id="蓝色的立方体"><a href="#蓝色的立方体" class="headerlink" title="蓝色的立方体"></a>蓝色的立方体</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;添加一个蓝色的立方体</span><br><span class="line">var blueBox &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Blue box&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-114.0, 35.0, 300000.0),</span><br><span class="line">  box: &#123;</span><br><span class="line">    dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0),</span><br><span class="line">    material: Cesium.Color.BLUE,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br><span class="line">viewer.zoomTo(blueBox);</span><br></pre></td></tr></table></figure><h2 id="具有黑色边框的立方体"><a href="#具有黑色边框的立方体" class="headerlink" title="具有黑色边框的立方体"></a>具有黑色边框的立方体</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;具有黑色边界的红色立方体   </span><br><span class="line">var redBox &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red box with black outline&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-107.0, 35.0, 300000.0),</span><br><span class="line">  box: &#123;</span><br><span class="line">    dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0),</span><br><span class="line">    material: Cesium.Color.RED.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.BLACK,</span><br><span class="line">outlineWidth: 3.0</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br><span class="line">viewer.zoomTo(redBox);</span><br></pre></td></tr></table></figure><h2 id="黄色立体边框"><a href="#黄色立体边框" class="headerlink" title="黄色立体边框"></a>黄色立体边框</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#x2F;&#x2F;黄色立方体边界</span><br><span class="line">var outlineOnly &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Yellow box outline&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-100.0, 35.0, 300000.0),</span><br><span class="line">  box: &#123;</span><br><span class="line">    dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0),</span><br><span class="line">    fill: false,</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.YELLOW,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br><span class="line">&#x2F;&#x2F;把所有的entity显示出来</span><br><span class="line">viewer.zoomTo(OutlineOnly);</span><br></pre></td></tr></table></figure><img src="/2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/1.png" class><h1 id="Circles-and-Ellipses"><a href="#Circles-and-Ellipses" class="headerlink" title="Circles and Ellipses"></a>Circles and Ellipses</h1><h2 id="带有边界的绿色圆"><a href="#带有边界的绿色圆" class="headerlink" title="带有边界的绿色圆"></a>带有边界的绿色圆</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;Green circle at height with outline</span><br><span class="line">&#x2F;&#x2F;带有边界的绿色圆</span><br><span class="line">var greenCircle &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-114.0, 30.0, 150000.0),</span><br><span class="line">  name: &quot;Green circle at height with outline&quot;,</span><br><span class="line">  ellipse: &#123;</span><br><span class="line">    semiMinorAxis: 300000.0,&#x2F;&#x2F;半短轴</span><br><span class="line">    semiMajorAxis: 300000.0,&#x2F;&#x2F;半长轴</span><br><span class="line">    height: 200000.0,&#x2F;&#x2F;距离地面高度</span><br><span class="line">    material: Cesium.Color.GREEN,&#x2F;&#x2F;填充材料</span><br><span class="line">    outline: true, &#x2F;&#x2F; height must be set for outline to display</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="位于地球表面的红色椭圆"><a href="#位于地球表面的红色椭圆" class="headerlink" title="位于地球表面的红色椭圆"></a>位于地球表面的红色椭圆</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;Red ellipse on surface</span><br><span class="line">&#x2F;&#x2F;位于地球表面的红色椭圆</span><br><span class="line">var redEllipse &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-107.0, 30.0),</span><br><span class="line">  name: &quot;Red ellipse on surface&quot;,</span><br><span class="line">  ellipse: &#123;</span><br><span class="line">    semiMinorAxis: 250000.0,</span><br><span class="line">    semiMajorAxis: 400000.0,</span><br><span class="line">    material: Cesium.Color.RED.withAlpha(0.5),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="蓝色柱体，旋转45°"><a href="#蓝色柱体，旋转45°" class="headerlink" title="蓝色柱体，旋转45°"></a>蓝色柱体，旋转45°</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">var blueEllipse &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-100.0, 30.0, 100000.0),</span><br><span class="line">  name: &quot;Blue translucent, rotated, and extruded ellipse with outline&quot;,</span><br><span class="line">  ellipse: &#123;</span><br><span class="line">    semiMinorAxis: 150000.0,</span><br><span class="line">    semiMajorAxis: 300000.0,</span><br><span class="line">    extrudedHeight: 200000.0,&#x2F;&#x2F;延伸高度</span><br><span class="line">    rotation: Cesium.Math.toRadians(45),&#x2F;&#x2F;旋转角度</span><br><span class="line">    material: Cesium.Color.BLUE.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><img src="/2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/2.png" class><h1 id="corridor"><a href="#corridor" class="headerlink" title="corridor"></a>corridor</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">var redCorridor &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red corridor on surface with rounded corners&quot;,</span><br><span class="line">  corridor: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -90.0,</span><br><span class="line">      46.0,</span><br><span class="line">      -95.0,</span><br><span class="line">      46.0,</span><br><span class="line">      -95.0,</span><br><span class="line">      41.0,</span><br><span class="line">    ]),</span><br><span class="line">    width: 200000.0,</span><br><span class="line">    material: Cesium.Color.RED.withAlpha(0.5),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">var greenCorridor &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Green corridor at height with mitered corners and outline&quot;,</span><br><span class="line">  corridor: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -90.0,</span><br><span class="line">      40.0,</span><br><span class="line">      -95.0,</span><br><span class="line">      40.0,</span><br><span class="line">      -95.0,</span><br><span class="line">      35.0,</span><br><span class="line">    ]),</span><br><span class="line">    height: 100000.0,</span><br><span class="line">    width: 200000.0,</span><br><span class="line">    cornerType: Cesium.CornerType.MITERED,</span><br><span class="line">    material: Cesium.Color.GREEN,</span><br><span class="line">    outline: true, &#x2F;&#x2F; height required for outlines to display</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">var blueCorridor &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Blue extruded corridor with beveled corners and outline&quot;,</span><br><span class="line">  corridor: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -80.0,</span><br><span class="line">      40.0,</span><br><span class="line">      -85.0,</span><br><span class="line">      40.0,</span><br><span class="line">      -85.0,</span><br><span class="line">      35.0,</span><br><span class="line">    ]),</span><br><span class="line">    height: 200000.0,</span><br><span class="line">    extrudedHeight: 100000.0,</span><br><span class="line">    width: 200000.0,</span><br><span class="line">    cornerType: Cesium.CornerType.BEVELED,</span><br><span class="line">    material: Cesium.Color.BLUE.withAlpha(0.5),</span><br><span class="line">    outline: true, &#x2F;&#x2F; height or extrudedHeight must be set for outlines to display</span><br><span class="line">    outlineColor: Cesium.Color.WHITE,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Cylinder-and-Cones"><a href="#Cylinder-and-Cones" class="headerlink" title="Cylinder and Cones"></a>Cylinder and Cones</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">var greenCylinder &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Green cylinder with black outline&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-100.0, 40.0, 200000.0),</span><br><span class="line">  cylinder: &#123;</span><br><span class="line">    length: 400000.0,&#x2F;&#x2F;高度（长度）</span><br><span class="line">    topRadius: 200000.0,&#x2F;&#x2F;上半径</span><br><span class="line">    bottomRadius: 200000.0,&#x2F;&#x2F;下半径</span><br><span class="line">    material: Cesium.Color.GREEN.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.DARK_GREEN,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var redCone &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red cone&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-100.0, 46.0, 200000.0),</span><br><span class="line">  cylinder: &#123;</span><br><span class="line">    length: 400000.0,</span><br><span class="line">    topRadius: 0.0,</span><br><span class="line">    bottomRadius: 200000.0,</span><br><span class="line">    material: Cesium.Color.RED,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Polygon"><a href="#Polygon" class="headerlink" title="Polygon"></a>Polygon</h1><h2 id="红色多边形"><a href="#红色多边形" class="headerlink" title="红色多边形"></a>红色多边形</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">var redPolygon &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red polygon on surface&quot;,</span><br><span class="line">  polygon: &#123;</span><br><span class="line">    hierarchy: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -115.0,</span><br><span class="line">      37.0,</span><br><span class="line">      -115.0,</span><br><span class="line">      32.0,</span><br><span class="line">      -107.0,</span><br><span class="line">      33.0,</span><br><span class="line">      -102.0,</span><br><span class="line">      31.0,</span><br><span class="line">      -102.0,</span><br><span class="line">      35.0,</span><br><span class="line">    ]),</span><br><span class="line">    material: Cesium.Color.RED,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="绿色延伸多边形"><a href="#绿色延伸多边形" class="headerlink" title="绿色延伸多边形"></a>绿色延伸多边形</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">var greenPolygon &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Green extruded polygon&quot;,</span><br><span class="line">  polygon: &#123;</span><br><span class="line">    hierarchy: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -108.0,</span><br><span class="line">      42.0,</span><br><span class="line">      -100.0,</span><br><span class="line">      42.0,</span><br><span class="line">      -104.0,</span><br><span class="line">      40.0,</span><br><span class="line">    ]),</span><br><span class="line">    extrudedHeight: 500000.0,</span><br><span class="line">    material: Cesium.Color.GREEN,</span><br><span class="line">    closeTop: true,&#x2F;&#x2F;布尔值，指定是否包含多边形的顶部</span><br><span class="line">    closeBottom: true,&#x2F;&#x2F;布尔值，指定是否包含多边形的底部</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="带有每个位置高度和轮廓的橙色多边形"><a href="#带有每个位置高度和轮廓的橙色多边形" class="headerlink" title="带有每个位置高度和轮廓的橙色多边形"></a>带有每个位置高度和轮廓的橙色多边形</h2><p>每个点的高度不一样</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">var orangePolygon &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Orange polygon with per-position heights and outline&quot;,</span><br><span class="line">  polygon: &#123;</span><br><span class="line">    hierarchy: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -108.0,</span><br><span class="line">      25.0,</span><br><span class="line">      100000,&#x2F;&#x2F;该点的高度</span><br><span class="line">      -100.0,</span><br><span class="line">      25.0,</span><br><span class="line">      100000,</span><br><span class="line">      -100.0,</span><br><span class="line">      30.0,</span><br><span class="line">      100000,</span><br><span class="line">      -108.0,</span><br><span class="line">      30.0,</span><br><span class="line">      300000,</span><br><span class="line">    ]),</span><br><span class="line">    extrudedHeight: 0,</span><br><span class="line">    perPositionHeight: true,</span><br><span class="line">    material: Cesium.Color.ORANGE.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.BLACK,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="中空的蓝色多边形"><a href="#中空的蓝色多边形" class="headerlink" title="中空的蓝色多边形"></a>中空的蓝色多边形</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">var bluePolygon &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Blue polygon with holes&quot;,</span><br><span class="line">  polygon: &#123;</span><br><span class="line">    hierarchy: &#123;</span><br><span class="line">      positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">        -99.0,</span><br><span class="line">        30.0,</span><br><span class="line">        -85.0,</span><br><span class="line">        30.0,</span><br><span class="line">        -85.0,</span><br><span class="line">        40.0,</span><br><span class="line">        -99.0,</span><br><span class="line">        40.0,</span><br><span class="line">      ]),</span><br><span class="line">      holes: [</span><br><span class="line">        &#123;</span><br><span class="line">          positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">            -97.0,</span><br><span class="line">            31.0,</span><br><span class="line">            -97.0,</span><br><span class="line">            39.0,</span><br><span class="line">            -87.0,</span><br><span class="line">            39.0,</span><br><span class="line">            -87.0,</span><br><span class="line">            31.0,</span><br><span class="line">          ]),</span><br><span class="line">          holes: [</span><br><span class="line">            &#123;</span><br><span class="line">              positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">                -95.0,</span><br><span class="line">                33.0,</span><br><span class="line">                -89.0,</span><br><span class="line">                33.0,</span><br><span class="line">                -89.0,</span><br><span class="line">                37.0,</span><br><span class="line">                -95.0,</span><br><span class="line">                37.0,</span><br><span class="line">              ]),</span><br><span class="line">              holes: [</span><br><span class="line">                &#123;</span><br><span class="line">                  positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">                    -93.0,</span><br><span class="line">                    34.0,</span><br><span class="line">                    -91.0,</span><br><span class="line">                    34.0,</span><br><span class="line">                    -91.0,</span><br><span class="line">                    36.0,</span><br><span class="line">                    -93.0,</span><br><span class="line">                    36.0,</span><br><span class="line">                  ]),</span><br><span class="line">                &#125;,</span><br><span class="line">              ],</span><br><span class="line">            &#125;,</span><br><span class="line">          ],</span><br><span class="line">        &#125;,</span><br><span class="line">      ],</span><br><span class="line">    &#125;,</span><br><span class="line">    material: Cesium.Color.BLUE.withAlpha(0.5),</span><br><span class="line">    height: 0,</span><br><span class="line">    outline: true, &#x2F;&#x2F; height is required for outline to display</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="带有每个位置高度和轮廓的青色垂直多边形"><a href="#带有每个位置高度和轮廓的青色垂直多边形" class="headerlink" title="带有每个位置高度和轮廓的青色垂直多边形"></a>带有每个位置高度和轮廓的青色垂直多边形</h2><p>每个点的参数有三个（分别是经度、纬度、高度）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">var cyanPolygon &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Cyan vertical polygon with per-position heights and outline&quot;,</span><br><span class="line">  polygon: &#123;</span><br><span class="line">    hierarchy: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -90.0,</span><br><span class="line">      41.0,</span><br><span class="line">      0.0,</span><br><span class="line">      -85.0,</span><br><span class="line">      41.0,</span><br><span class="line">      500000.0,</span><br><span class="line">      -80.0,</span><br><span class="line">      41.0,</span><br><span class="line">      0.0,</span><br><span class="line">    ]),</span><br><span class="line">    perPositionHeight: true,</span><br><span class="line">    material: Cesium.Color.CYAN.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.BLACK,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="有轮廓的菱形线的紫色多边形"><a href="#有轮廓的菱形线的紫色多边形" class="headerlink" title="有轮廓的菱形线的紫色多边形"></a>有轮廓的菱形线的紫色多边形</h2><p>outline边框线的类型是虚线，而非实线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">var purplePolygonUsingRhumbLines &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Purple polygon using rhumb lines with outline&quot;,</span><br><span class="line">  polygon: &#123;</span><br><span class="line">    hierarchy: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -120.0,</span><br><span class="line">      45.0,</span><br><span class="line">      -80.0,</span><br><span class="line">      45.0,</span><br><span class="line">      -80.0,</span><br><span class="line">      55.0,</span><br><span class="line">      -120.0,</span><br><span class="line">      55.0,</span><br><span class="line">    ]),</span><br><span class="line">    extrudedHeight: 50000,</span><br><span class="line">    material: Cesium.Color.PURPLE,</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.MAGENTA,</span><br><span class="line">    arcType: Cesium.ArcType.RHUMB,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><img src="/2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/3.png" class><h1 id="PolyLines"><a href="#PolyLines" class="headerlink" title="PolyLines"></a>PolyLines</h1><ul><li><p>arcType：获取或设置ArcType属性，该属性指定线段应该是大圆弧、直角线还是线性连接的，ArcType定义连接顶点应采用的路径。</p><ul><li>有三个路径NONE、GEODESIC、RHUMB。</li><li>RHUMB：恒向线是始终与经线保持相同的角度；</li></ul></li><li><p>clampToGround：获取或设置布尔属性，该布尔属性指定是否应将折线固定在地面上。默认为false。</p></li><li><p>position：位置</p></li><li><p>width：宽度</p></li><li><p>material：这个参数可以通过调用函数来丰富内容，函数的种类包括多种，可以是虚线、发光的线等等</p><ul><li>函数名的例子：Polyline<strong>Glow</strong>MaterialProperty，Glow替换掉其他的即可；</li></ul></li></ul><h2 id="在地形上的红色线"><a href="#在地形上的红色线" class="headerlink" title="在地形上的红色线"></a>在地形上的红色线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">var redLine &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red line on terrain&quot;,</span><br><span class="line">  polyline: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([-75, 35, -125, 35]),</span><br><span class="line">    width: 5,</span><br><span class="line">    material: Cesium.Color.RED,</span><br><span class="line">    clampToGround: true,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="绿色恒向线"><a href="#绿色恒向线" class="headerlink" title="绿色恒向线"></a>绿色恒向线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">var greenRhumbLine &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Green rhumb line&quot;,</span><br><span class="line">  polyline: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([-75, 35, -125, 35]),</span><br><span class="line">    width: 5,</span><br><span class="line">    arcType: Cesium.ArcType.RHUMB,</span><br><span class="line">    material: Cesium.Color.GREEN,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="在表面发光的蓝线"><a href="#在表面发光的蓝线" class="headerlink" title="在表面发光的蓝线"></a>在表面发光的蓝线</h2><ul><li>PolylineGlowMaterialProperty该函数有三个参数，分别是</li><li>color：颜色      </li><li>glowPower：用于指定发光强度     </li><li>taperPower：指定渐缩效果的强度，以占总线长的百分比表示。如果为1.0或更高，则不使用锥度效果。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">var glowingLine &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Glowing blue line on the surface&quot;,</span><br><span class="line">  polyline: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([-75, 37, -125, 37]),</span><br><span class="line">    width: 10,</span><br><span class="line">    material: new Cesium.PolylineGlowMaterialProperty(&#123;</span><br><span class="line">  &#x2F;&#x2F;PolylineGlowMaterialProperty该函数有三个参数，分别是</span><br><span class="line">  &#x2F;&#x2F;color：颜色</span><br><span class="line">  &#x2F;&#x2F;glowPower：用于指定发光强度</span><br><span class="line">  &#x2F;&#x2F;taperPower：指定渐缩效果的强度，以占总线长的百分比表示。如果为1.0或更高，则不使用锥度效果。</span><br><span class="line">      glowPower: 0.2,</span><br><span class="line">      taperPower: 0.5,</span><br><span class="line">      color: Cesium.Color.CORNFLOWERBLUE,</span><br><span class="line">    &#125;),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="带有黑色轮廓的橙色线，具有一定的高度并且沿着表面"><a href="#带有黑色轮廓的橙色线，具有一定的高度并且沿着表面" class="headerlink" title="带有黑色轮廓的橙色线，具有一定的高度并且沿着表面"></a>带有黑色轮廓的橙色线，具有一定的高度并且沿着表面</h2><ul><li>PolylineOutlineMaterialProperty函数有三个参数：color颜色、outlineColor轮廓颜色、outlineWidth轮廓宽度</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">var orangeOutlined &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name:</span><br><span class="line">    &quot;Orange line with black outline at height and following the surface&quot;,</span><br><span class="line">  polyline: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -75,</span><br><span class="line">      39,</span><br><span class="line">      250000,</span><br><span class="line">      -125,</span><br><span class="line">      39,</span><br><span class="line">      250000,</span><br><span class="line">    ]),</span><br><span class="line">    width: 5,</span><br><span class="line">    material: new Cesium.PolylineOutlineMaterialProperty(&#123;</span><br><span class="line">  &#x2F;&#x2F;PolylineOutlineMaterialProperty函数有三个参数：color、outlineColor、outlineWidth</span><br><span class="line">      color: Cesium.Color.ORANGE,</span><br><span class="line">      outlineWidth: 2,</span><br><span class="line">      outlineColor: Cesium.Color.BLACK,</span><br><span class="line">    &#125;),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="具有一定高度的紫色的直箭头"><a href="#具有一定高度的紫色的直箭头" class="headerlink" title="具有一定高度的紫色的直箭头"></a>具有一定高度的紫色的直箭头</h2><ul><li>PolylineArrowMaterialProperty只有一个color函数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">var purpleArrow &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Purple straight arrow at height&quot;,</span><br><span class="line">  polyline: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -75,</span><br><span class="line">      43,</span><br><span class="line">      500000,</span><br><span class="line">      -125,</span><br><span class="line">      43,</span><br><span class="line">      500000,</span><br><span class="line">    ]),</span><br><span class="line">    width: 10,</span><br><span class="line">    arcType: Cesium.ArcType.NONE,</span><br><span class="line">    material: new Cesium.PolylineArrowMaterialProperty(</span><br><span class="line">  &#x2F;&#x2F;PolylineArrowMaterialProperty只有一个color函数</span><br><span class="line">      Cesium.Color.PURPLE</span><br><span class="line">    ),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="蓝色虚线"><a href="#蓝色虚线" class="headerlink" title="蓝色虚线"></a>蓝色虚线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">var dashedLine &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Blue dashed line&quot;,</span><br><span class="line">  polyline: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -75,</span><br><span class="line">      45,</span><br><span class="line">      500000,</span><br><span class="line">      -125,</span><br><span class="line">      45,</span><br><span class="line">      500000,</span><br><span class="line">    ]),</span><br><span class="line">    width: 4,</span><br><span class="line">    material: new Cesium.PolylineDashMaterialProperty(&#123;</span><br><span class="line">  &#x2F;&#x2F;PolylineDashMaterialProperty函数有四个参数，分别是：</span><br><span class="line">  &#x2F;&#x2F;color</span><br><span class="line">  &#x2F;&#x2F;gapColor：间隙的颜色</span><br><span class="line">  &#x2F;&#x2F;dashLength：虚线图案的长度（像素）</span><br><span class="line">  &#x2F;&#x2F;dashPattern：指定破折号的16位模式</span><br><span class="line">      color: Cesium.Color.CYAN,</span><br><span class="line">    &#125;),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><img src="/2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/4.png" class><h1 id="Polyline-Volumes"><a href="#Polyline-Volumes" class="headerlink" title="Polyline Volumes"></a>Polyline Volumes</h1><p>待定</p><img src="/2020/12/17/Cesium%E4%B8%AD%E7%9A%84Entities/5.png" class><h1 id="Rectangle"><a href="#Rectangle" class="headerlink" title="Rectangle"></a>Rectangle</h1><h2 id="红色矩形"><a href="#红色矩形" class="headerlink" title="红色矩形"></a>红色矩形</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">var redRectangle &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red translucent rectangle&quot;,</span><br><span class="line">  rectangle: &#123;</span><br><span class="line">    coordinates: Cesium.Rectangle.fromDegrees(</span><br><span class="line">&#x2F;&#x2F;只需要给出两点参数即可，两点为对角线的点</span><br><span class="line">      -110.0,</span><br><span class="line">      20.0,</span><br><span class="line">      -80.0,</span><br><span class="line">      25.0</span><br><span class="line">    ),</span><br><span class="line">    material: Cesium.Color.RED.withAlpha(0.5),</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="绿色半透明，旋转，和挤压矩形具有高度和轮廓"><a href="#绿色半透明，旋转，和挤压矩形具有高度和轮廓" class="headerlink" title="绿色半透明，旋转，和挤压矩形具有高度和轮廓"></a>绿色半透明，旋转，和挤压矩形具有高度和轮廓</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">var greenRectangle &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name:</span><br><span class="line">    &quot;Green translucent, rotated, and extruded rectangle at height with outline&quot;,</span><br><span class="line">  rectangle: &#123;</span><br><span class="line">    coordinates: Cesium.Rectangle.fromDegrees(</span><br><span class="line">      -110.0,</span><br><span class="line">      30.0,</span><br><span class="line">      -100.0,</span><br><span class="line">      40.0</span><br><span class="line">    ),</span><br><span class="line">    material: Cesium.Color.GREEN.withAlpha(0.5),</span><br><span class="line">    rotation: Cesium.Math.toRadians(45),</span><br><span class="line">    extrudedHeight: 300000.0,</span><br><span class="line">    height: 100000.0,</span><br><span class="line">    outline: true, &#x2F;&#x2F; height must be set for outline to display</span><br><span class="line">    outlineColor: Cesium.Color.BLACK,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">var rotation &#x3D; Cesium.Math.toRadians(30);</span><br><span class="line"></span><br><span class="line">function getRotationValue() &#123;</span><br><span class="line">  &#x2F;&#x2F;旋转角度每次增加0.05</span><br><span class="line">  rotation +&#x3D; 0.005;</span><br><span class="line">  return rotation;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="旋转矩形"><a href="#旋转矩形" class="headerlink" title="旋转矩形"></a>旋转矩形</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Rotating rectangle with rotating texture coordinate&quot;,</span><br><span class="line">  rectangle: &#123;</span><br><span class="line">    coordinates: Cesium.Rectangle.fromDegrees(-92.0, 30.0, -76.0, 40.0),</span><br><span class="line">    material: Cesium.Color.RED,</span><br><span class="line">    rotation: new Cesium.CallbackProperty(getRotationValue, false),</span><br><span class="line">    stRotation: new Cesium.CallbackProperty(getRotationValue, false),</span><br><span class="line">    classificationType: Cesium.ClassificationType.TERRAIN,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Spheres-and-Ellipsoids"><a href="#Spheres-and-Ellipsoids" class="headerlink" title="Spheres and Ellipsoids"></a>Spheres and Ellipsoids</h1><ul><li>确定一个位置，然后增加一个半径</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">var blueEllipsoid &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Blue ellipsoid&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-114.0, 40.0, 300000.0),</span><br><span class="line">  ellipsoid: &#123;</span><br><span class="line">&#x2F;&#x2F;半径</span><br><span class="line">    radii: new Cesium.Cartesian3(200000.0, 200000.0, 300000.0),</span><br><span class="line">    material: Cesium.Color.BLUE,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var redSphere &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red sphere with black outline&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-107.0, 40.0, 300000.0),</span><br><span class="line">  ellipsoid: &#123;</span><br><span class="line">    radii: new Cesium.Cartesian3(300000.0, 300000.0, 300000.0),</span><br><span class="line">    material: Cesium.Color.RED.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.BLACK,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">var outlineOnly &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Yellow ellipsoid outline&quot;,</span><br><span class="line">  position: Cesium.Cartesian3.fromDegrees(-100.0, 40.0, 300000.0),</span><br><span class="line">  ellipsoid: &#123;</span><br><span class="line">    radii: new Cesium.Cartesian3(200000.0, 200000.0, 300000.0),</span><br><span class="line">    fill: false,</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.YELLOW,</span><br><span class="line">    slicePartitions: 24,</span><br><span class="line">    stackPartitions: 36,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Walls"><a href="#Walls" class="headerlink" title="Walls"></a>Walls</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">var redWall &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Red wall at height&quot;,</span><br><span class="line">  wall: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -115.0,</span><br><span class="line">      44.0,</span><br><span class="line">      200000.0,</span><br><span class="line">      -90.0,</span><br><span class="line">      44.0,</span><br><span class="line">      200000.0,</span><br><span class="line">    ]),</span><br><span class="line">    minimumHeights: [100000.0, 100000.0],</span><br><span class="line">    material: Cesium.Color.RED,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">var greenWall &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Green wall from surface with outline&quot;,</span><br><span class="line">  wall: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArrayHeights([</span><br><span class="line">      -107.0,</span><br><span class="line">      43.0,</span><br><span class="line">      100000.0,</span><br><span class="line">      -97.0,</span><br><span class="line">      43.0,</span><br><span class="line">      100000.0,</span><br><span class="line">      -97.0,</span><br><span class="line">      40.0,</span><br><span class="line">      100000.0,</span><br><span class="line">      -107.0,</span><br><span class="line">      40.0,</span><br><span class="line">      100000.0,</span><br><span class="line">      -107.0,</span><br><span class="line">      43.0,</span><br><span class="line">      100000.0,</span><br><span class="line">    ]),</span><br><span class="line">    material: Cesium.Color.GREEN,</span><br><span class="line">    outline: true,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">var blueWall &#x3D; viewer.entities.add(&#123;</span><br><span class="line">  name: &quot;Blue wall with sawtooth heights and outline&quot;,</span><br><span class="line">  wall: &#123;</span><br><span class="line">    positions: Cesium.Cartesian3.fromDegreesArray([</span><br><span class="line">      -115.0,</span><br><span class="line">      50.0,</span><br><span class="line">      -112.5,</span><br><span class="line">      50.0,</span><br><span class="line">      -110.0,</span><br><span class="line">      50.0,</span><br><span class="line">      -107.5,</span><br><span class="line">      50.0,</span><br><span class="line">      -105.0,</span><br><span class="line">      50.0,</span><br><span class="line">      -102.5,</span><br><span class="line">      50.0,</span><br><span class="line">      -100.0,</span><br><span class="line">      50.0,</span><br><span class="line">      -97.5,</span><br><span class="line">      50.0,</span><br><span class="line">      -95.0,</span><br><span class="line">      50.0,</span><br><span class="line">      -92.5,</span><br><span class="line">      50.0,</span><br><span class="line">      -90.0,</span><br><span class="line">      50.0,</span><br><span class="line">    ]),</span><br><span class="line">    maximumHeights: [</span><br><span class="line">      100000,</span><br><span class="line">      200000,</span><br><span class="line">      100000,</span><br><span class="line">      200000,</span><br><span class="line">      100000,</span><br><span class="line">      200000,</span><br><span class="line">      100000,</span><br><span class="line">      200000,</span><br><span class="line">      100000,</span><br><span class="line">      200000,</span><br><span class="line">      100000,</span><br><span class="line">    ],</span><br><span class="line">    minimumHeights: [</span><br><span class="line">      0,</span><br><span class="line">      100000,</span><br><span class="line">      0,</span><br><span class="line">      100000,</span><br><span class="line">      0,</span><br><span class="line">      100000,</span><br><span class="line">      0,</span><br><span class="line">      100000,</span><br><span class="line">      0,</span><br><span class="line">      100000,</span><br><span class="line">      0,</span><br><span class="line">    ],</span><br><span class="line">    material: Cesium.Color.BLUE.withAlpha(0.5),</span><br><span class="line">    outline: true,</span><br><span class="line">    outlineColor: Cesium.Color.BLACK,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Cesium </tag>
            
            <tag> Entity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文-深度学习综述</title>
      <link href="2020/11/18/%E8%AE%BA%E6%96%87-DeepLearning%E7%BB%BC%E8%BF%B0/"/>
      <url>2020/11/18/%E8%AE%BA%E6%96%87-DeepLearning%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. “<strong>Deep learning</strong>.” Nature 521.7553 (2015): 436-444. [<a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf">pdf]</a> <strong>(Three Giants’ Survey)</strong> </p></blockquote><p>深度学习允许由多个处理层组成的计算模型学习具有多个抽象层次的数据表示。这些方法极大地提高了语音识别、视觉对象识别、对象检测和许多其他领域(如药物发现和基因组学)的技术水平。深度学习通过使用反向传播算法指示机器应该如何改变其内部参数来发现大型数据集中复杂的结构，该内部参数用于从前一层的表示计算每一层的表示。深度<strong>卷积网络</strong>在处理图像、视频、语音和音频方面带来了突破，而<strong>循环网络</strong>则照亮了文本和语音等连续数据。</p><h1 id="The-difference-between-Machie-learning-and-Deep-Learning"><a href="#The-difference-between-Machie-learning-and-Deep-Learning" class="headerlink" title="The difference between Machie learning and Deep Learning"></a>The difference between Machie learning and Deep Learning</h1><p>传统的机器学习技术在处理原始形式的自然数据方面能力有限。几十年来，构建模式识别或机器学习系统需要仔细的工程设计和大量的领域专业知识来设计特征提取器；</p><p>表示学习是一套方法，它允许机器获得原始数据，并自动发现检测或分类所需的表示。深度学习方法是具有多层次表示的表示学习方法，通过组合简单但非线性的模块获得，每个模块将一个层次的表示(从原始输入开始)转换为更高、稍微抽象的层次的表示。对于分类任务，更高层次的表征放大了对辨别和抑制无关变化很重要的输入方面。对于分类任务，更高层次的表征放大了对辨别和抑制无关变化很重要的输入方面。<strong>深度学习的关键方面是这些特征层不是由人类工程师设计的:它们是使用通用学习程序从数据中学习的。</strong></p><p>深度学习在不久的将来会有更多的成功，因为它只需要很少的手工工程，所以它可以很容易地利用可用计算和数据量的增加。目前正在为深度神经网络开发的新学习算法和架构只会加速这一进展。</p><h1 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h1><p>机器学习最常见的形式，不管深入与否，都是监督学习。以图像分类为例，首先需要收集大量数据，在训练过程中，机器会显示一个图像，并以分数向量的形式产生输出，每个类别一个分数。我们希望期望的类别在所有类别中得分最高，但这不太可能在训练前发生。我们计算一个<strong>目标函数，</strong>该函数测量输出分数和期望分数模式之间的误差(或距离)。然后，机器修改其内部可调参数，以减少这一误差。这些可调参数通常被称为<strong>权重</strong>，是实数，可以被视为定义机器输入输出功能的“旋钮”。在一个典型的深度学习系统中，可能有上亿个这样的可调权重，以及上亿个用来训练机器的标记例子。为了适当地调整权重向量，学习算法<strong>计算梯度向量</strong>，对于每个权重，梯度向量指示如果权重增加很小的量，误差将增加或减少多少。权重向量随后被调整到与梯度向量相反的方向。</p><p>在实践中，大多数从业者使用一种叫做随机梯度下降(SGD)的方法。这包括显示几个示例的输入向量，计算输出和误差，计算这些示例的平均梯度，并相应地调整权重。对训练集中的许多小样本集重复该过程，直到目标函数的平均值停止下降。之所以称之为随机，是因为每一个小例子都给出了所有例子的平均梯度的噪声估计。<strong>与复杂得多的优化技术相比，这个简单的过程通常能惊人地快速找到一组好的权重</strong>。经过培训后，系统的性能在一组不同的例子上进行测量，称为测试集。这用来测试机器的泛化能力——它对训练中从未见过的新输入产生合理答案的能力。当前机器学习的许多实际应用在手工设计的特征上使用线性分类器。两类线性分类器计算特征向量分量的加权和。如果加权和高于阈值，则输入被分类为属于特定类别。</p><p>浅层分类器需要一个好的特征提取器来解决选择性不变性的困境——<strong>一个产生对图像中对辨别很重要的方面有选择性的表示，但对不相关的方面如动物的姿势不变的表示</strong>。为了使分类器更强大，可以使用通用的非线性特征，如核方法，但是通用的特征，如高斯核产生的特征，不允许学习者远离训练示例进行概括。<strong>传统的选择是</strong>手工设计好的特征提取器，这需要大量的工程技术和领域专业知识。但是如果使用通用的学习程序可以自动学习好的特性，这一切都可以避免。<strong>这是深度学习的关键优势。</strong></p><p><strong>深度学习架构是简单模块的多层堆栈，所有(或大部分)模块都需要学习，其中许多模块计算非线性输入输出映射。堆栈中的每个模块转换其输入，以增加表示的选择性和不变性。有了多个非线性层，比如深度为5到20，系统可以实现极其复杂的输入功能，同时对微小的细节敏感(区分萨摩耶和白狼)，对背景、姿势、照明和周围对象等不相关的大变化不敏感。</strong></p><h1 id="Backpropagation-to-train-multilayer-architectures"><a href="#Backpropagation-to-train-multilayer-architectures" class="headerlink" title="Backpropagation to train multilayer architectures"></a>Backpropagation to train multilayer architectures</h1><p>多层架构可以通过简单的随机梯度下降来训练。只要模块是其输入和内部权重的相对平滑的函数，就可以使用反向传播过程来计算梯度。计算目标函数相对于多层堆叠模块的<strong>权重的梯度的反向传播过程</strong>只不过是导数链式法则的实际应用。关键的见解是，目标相对于模块输入的导数(或梯度)可以通过从相对于该模块输出(或后续模块输入)的梯度向后工作来计算。反向传播方程可以重复应用，以通过所有模块传播梯度，从顶部的输出(网络产生其预测)一直到底部(外部输入被馈送)。一旦计算出这些梯度，就可以直接计算每个模块权重的梯度。</p><p>目前最流行的非线性函数是**整流线性单元(ReLU)**，简单来说就是半波整流器f(z) =  max(z，0)。在过去的几十年里，神经网络使用更平滑的非线性，例如tanh(z)或1/(1+exp(z))，但是ReLU通常在具有许多层的网络中学习得更快，允许在没有无监督预训练的情况下训练深度监督网络28。</p><p>人们普遍认为，学习有用的、多阶段的、几乎没有先验知识的特征提取器是不可行的。特别是，人们普遍认为简单的梯度下降会陷入较差的<strong>局部极小值——权重配置</strong>，对于这种配置，任何微小的变化都不会降低平均误差。在实践中，差的局部极小值很少是大网络的问题。不管初始条件如何，系统几乎总是能得到质量非常相似的解。最近的理论和经验结果强烈表明，局部最小值一般来说不是一个严重的问题。取而代之的是，地形被大量的鞍点组合在一起，其中坡度为零，表面在大多数维度上向上弯曲，而在大多数维度上向下弯曲</p><h1 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h1><p>卷积神经网络被设计为处理以多阵列形式出现的数据，例如由三个2D阵列组成的彩色图像，包含三个颜色通道中的像素强度。许多数据形式是多阵列的形式:信号和序列的1D，包括语言；2D用于图像或音频频谱图；和3D用于视频或体积图像。利用自然信号特性的ConvNets背后有四个关键思想:<strong>本地连接、共享权重、池化和使用多个层。</strong></p><p>前几个阶段由两种类型的层组成:卷积层和汇集层。卷积层中的单元被组织在特征图中，其中每个单元通过一组称为滤波器组的权重与前一层的特征图中的局部片相连。这个局部加权和的结果然后通过一个非线性，如ReLU。要素地图中的所有单元共享同一个滤波器组。一个图层中的不同要素地图使用不同的滤波器组。这种架构有两个原因。首先，在图像等阵列数据中，局部值组通常高度相关，形成易于检测的独特局部图案。第二，图像和其他信号的局部统计对于位置是不变的。换句话说，如果一个图案可以出现在图像的一部分，它可以出现在任何地方，因此不同位置的单元共享相同的权重，并在阵列的不同部分检测相同的图案。</p><p>一个典型的池单元计算一个要素地图(或几个要素地图)中单元的局部斑块的最大值。相邻的汇集单元从移位超过一行或一列的块中获取输入，从而降低表示的维数，并创建对小移位和失真的不变性。卷积、非线性和汇集的两个或三个阶段被堆叠，随后是更多的卷积和全连接层。</p><p>深度神经网络利用了许多自然信号是组成层次的特性，其中较高级别的特征是通过组成较低级别的特征获得的。在图像中，边缘的局部组合形成图案，图案组合成零件，零件形成物体。从声音到音素、音素、音节、单词和句子，语音和文本中存在类似的层次结构。当前一层中的元素在位置和外观上发生变化时，池化允许表示变化很小。</p><h1 id="Image-understanding-with-deep-convolutional-networks"><a href="#Image-understanding-with-deep-convolutional-networks" class="headerlink" title="Image understanding with deep convolutional networks"></a>Image understanding with deep convolutional networks</h1><p>在2012年ImageNet竞赛之前，ConvNets在很大程度上被主流计算机视觉和机器学习社区所抛弃。当深度卷积网络应用于包含1000个不同类别的约100万幅网络图像的数据集时，它们取得了惊人的结果，几乎将最佳竞争方法的错误率减半。<strong>这一成功来自于GPU、ReLUs(一种称为dropout的新正则化技术)和通过变形现有样本来生成更多训练样本的技术的有效使用。这</strong>一成功带来了计算机视觉的革命；ConvNets现在是几乎所有识别和检测任务的主要方法，并接近人类在某些任务上的表现。</p><h1 id="Distributed-representations-and-language-processing"><a href="#Distributed-representations-and-language-processing" class="headerlink" title="Distributed representations and language processing"></a>Distributed representations and language processing</h1><p>这一部分回看论文即可</p><h1 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h1><p>RNNs一旦及时展开(图5)，可以被视为非常深的前馈网络，其中所有层共享相同的权重。虽然他们的主要目的是学习长期依赖性，但理论和经验证据表明，很难学会将信息存储很长时间78。为了纠正这一点，一个想法是用一个明确的内存来扩充网络。这种类型的第一个提议是使用特殊隐藏单元的长短期记忆(LSTM)网络，其自然行为是长时间记住输入79。一种叫做存储单元的特殊单元就像累加器或门控泄漏神经元一样:它在下一个权重为1的时间步与自己相连，因此它复制自己的实值状态并累积外部信号，但这种自连接被另一个单元多重门控，该单元学习决定何时清除存储器内容。LSTM网络随后被证明比传统的神经网络更有效，尤其是当它们在每个时间步长有几层时，使得整个语音识别系统能够从声学一直到转录中的字符序列</p><img src="/2020/11/18/%E8%AE%BA%E6%96%87-DeepLearning%E7%BB%BC%E8%BF%B0/1.jpg" class>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Paper Recurrence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN——时序数据的采样</title>
      <link href="2020/11/16/DL-RNN%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E6%A0%B7/"/>
      <url>2020/11/16/DL-RNN%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E6%A0%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="语言模型数据集"><a href="#语言模型数据集" class="headerlink" title="语言模型数据集"></a>语言模型数据集</h1><h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&#x27;../data/jaychou_lyrics.txt.zip&#x27;</span>) <span class="keyword">as</span> zin:</span><br><span class="line">    <span class="keyword">with</span> zin.open(<span class="string">&#x27;jaychou_lyrics.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        corpus_chars = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">corpus_chars[:<span class="number">40</span>]</span><br></pre></td></tr></table></figure><pre><code>&#39;想要有直升机\n想要和你飞到宇宙去\n想要和你融化在一起\n融化在宇宙里\n我每天每天每&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">corpus_chars = corpus_chars.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\t&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line">corpus_chars[<span class="number">0</span>:<span class="number">49</span>]</span><br></pre></td></tr></table></figure><pre><code>&#39;想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你 &#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">idx_to_char = list(set(corpus_chars)) <span class="comment">#去重复，求得不重复的字符</span></span><br><span class="line">char_to_idx = dict([(char, i)<span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])<span class="comment"># 将字符与索引一一映射构造字典</span></span><br><span class="line">vocab_size = len(char_to_idx)</span><br><span class="line">print(vocab_size)</span><br><span class="line">print(char_to_idx[<span class="string">&#x27;有&#x27;</span>])</span><br><span class="line"><span class="comment">#print(idx_to_char)</span></span><br></pre></td></tr></table></figure><pre><code>1027596</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">print(<span class="string">&#x27;chars:&#x27;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">print(<span class="string">&#x27;indices:&#x27;</span>, sample)</span><br></pre></td></tr></table></figure><pre><code>chars: 想要有直升机 想要和你飞到宇宙去 想要和indices: [63, 155, 596, 58, 143, 855, 670, 63, 155, 607, 151, 597, 227, 181, 161, 459, 670, 63, 155, 607]</code></pre><h2 id="时序数据的采样"><a href="#时序数据的采样" class="headerlink" title="时序数据的采样"></a>时序数据的采样</h2><p>在训练过程中，我们每次需要随机读取小批量的样本x和标签值y，时序数据的一个样本通常包含多个连续的字符。以上文数据“<strong>想要有直升机 想要和你飞到宇宙去 想要和</strong>”为例，假设时间步数为5，样本序列中就会有五个字符——“<strong>想要有直升</strong>”，而标签序列的字符为样本字符在训练集中的下一个字符——“<strong>要有直升机</strong>”。</p><p>对于时序数据采样，我们有两种方法，随机采样和相邻采样</p><h2 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h2><p>batch_size为每个小批量的样本数，num_steps为单个样本的时间步数，随机采样中，每个样本为原始序列中任意截取长度为num_steps的一段序列，相邻的两个小批量在原始序列的位置并不一定相邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span>(<span class="params">corpus_indices, batch_size, num_steps, ctx=None</span>):</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps <span class="comment">#下取整，得到的是总样本个数</span></span><br><span class="line">    epoch_size = num_examples // batch_size <span class="comment">#总共有多少个小批量样本，每个小批量样本个数为batch_size</span></span><br><span class="line"></span><br><span class="line">    example_indices = list(range(num_examples))<span class="comment">#打乱顺序</span></span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span>(<span class="params">pos</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i: i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_seq = list(range(<span class="number">40</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y, <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>X:  [[ 6.  7.  8.  9. 10. 11.] [18. 19. 20. 21. 22. 23.]]&lt;NDArray 2x6 @cpu(0)&gt; Y: [[ 7.  8.  9. 10. 11. 12.] [19. 20. 21. 22. 23. 24.]]&lt;NDArray 2x6 @cpu(0)&gt; X:  [[24. 25. 26. 27. 28. 29.] [ 0.  1.  2.  3.  4.  5.]]&lt;NDArray 2x6 @cpu(0)&gt; Y: [[25. 26. 27. 28. 29. 30.] [ 1.  2.  3.  4.  5.  6.]]&lt;NDArray 2x6 @cpu(0)&gt; X:  [[12. 13. 14. 15. 16. 17.] [30. 31. 32. 33. 34. 35.]]&lt;NDArray 2x6 @cpu(0)&gt; Y: [[13. 14. 15. 16. 17. 18.] [31. 32. 33. 34. 35. 36.]]&lt;NDArray 2x6 @cpu(0)&gt; 以这个输入的示例为例，针对单个X，每个X包含batch_size=2个批量的样本，每个样本包含num_steps=6个字符，而Y的每个样本为X每个样本的后一个字符；然后我们比较相邻的两个X，会发现第一个X的首字符为６，而第二个X的首字符为24，两者不相邻。同理我们可以比较相邻采样，这样就会更加容易理解。</code></pre><h2 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h2><p>除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span>(<span class="params">corpus_indices, batch_size, num_steps, ctx=None</span>):</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices) <span class="comment">#字符长度</span></span><br><span class="line">    batch_len = data_len // batch_size<span class="comment">#可以有多少个小批量</span></span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>: batch_size*batch_len].reshape((batch_size, batch_len))<span class="comment">#转换维度batch_size行，batch_len列</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_consecutive(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y, <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>X:  [[ 0.  1.  2.  3.  4.  5.] [20. 21. 22. 23. 24. 25.]]&lt;NDArray 2x6 @cpu(0)&gt; Y: [[ 1.  2.  3.  4.  5.  6.] [21. 22. 23. 24. 25. 26.]]&lt;NDArray 2x6 @cpu(0)&gt; X:  [[ 6.  7.  8.  9. 10. 11.] [26. 27. 28. 29. 30. 31.]]&lt;NDArray 2x6 @cpu(0)&gt; Y: [[ 7.  8.  9. 10. 11. 12.] [27. 28. 29. 30. 31. 32.]]&lt;NDArray 2x6 @cpu(0)&gt; X:  [[12. 13. 14. 15. 16. 17.] [32. 33. 34. 35. 36. 37.]]&lt;NDArray 2x6 @cpu(0)&gt; Y: [[13. 14. 15. 16. 17. 18.] [33. 34. 35. 36. 37. 38.]]&lt;NDArray 2x6 @cpu(0)&gt; </code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN经典论文—-NiN模型</title>
      <link href="2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="NiNe同AlexNet和VGG网络的区别"><a href="#NiNe同AlexNet和VGG网络的区别" class="headerlink" title="NiNe同AlexNet和VGG网络的区别"></a>NiNe同AlexNet和VGG网络的区别</h1><img src="/2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/1.png" class><h1 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h1><p>卷积窗口形状为1×1（kh=kw=1）的多通道卷积层。我们通常称之为<strong>1×1卷积层，</strong>并将其中的卷积运算称为1×1卷积。因为使用了最小窗口，1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能，其主要计算发生在通道维上。下图展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本那么1×1卷积层的作用与全连接层等价。</p><img src="/2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/2.png" class><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><ul><li><p>传统的convolution层</p><p>传统的卷积层，通过卷积核得到</p><img src="/2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/3.png" class></li><li><p>单通道mlpconv层</p><img src="/2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/4.png" class></li><li><p>跨通道的mlpconv</p><img src="/2020/11/12/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94-NiN%E6%A8%A1%E5%9E%8B/5.png" class></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Paper Recurrence </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN经典算法——VGG模型</title>
      <link href="2020/11/10/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94VGG%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/11/10/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94VGG%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="VGG原理"><a href="#VGG原理" class="headerlink" title="VGG原理"></a>VGG原理</h1><img src="/2020/11/10/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94VGG%E6%A8%A1%E5%9E%8B/1.png" class><p>VGG有两种结构，分别是VGG16（上图D列）和VGG19（上图E列)，两者并没有本质上的区别，只是网络深度不一样。</p><p>VGG16相比AlexNet的一个改进是<strong>采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）</strong>。VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。</p><p>两个3x3的卷积层串联相当于1个5x5的卷积层，即一个像素会跟周围5x5的像素产生关联，可以说感受野大小为5x5。而3个3x3的卷积层串联的效果则相当于1个7x7的卷积层;</p><p>参数：</p><p><a href="https://zhuanlan.zhihu.com/p/41423739">https://zhuanlan.zhihu.com/p/41423739</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Paper Recurrence </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN经典算法——AlexNet模型</title>
      <link href="2020/11/10/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94AlexNet%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/11/10/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94AlexNet%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky [1]。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。</p><img src="/2020/11/10/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94AlexNet%E6%A8%A1%E5%9E%8B/1.png" class><p>第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</p><ul><li>AlexNet第一层中的卷积窗口形状是11×11，strides=4。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。</li><li>第二层中的卷积窗口形状减小到5×5；第三、四、五层的卷积窗口大小为3x3.</li><li>第一、第二和第五个卷积层之后都使用了窗口形状为3×3、步幅为2的最大池化层；而第三、四个卷积层之后没有池化层。</li></ul><p>第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。</p><ul><li>一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。</li><li>另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</li></ul><p>第三，AlexNet通过丢弃法来控制全连接层的模型复杂度。</p><p>第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p><p>参考：</p><p>《动手学深度学习》</p><p><a href="https://blog.csdn.net/dan_teng/article/details/87192430">https://blog.csdn.net/dan_teng/article/details/87192430</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Paper Recurrence </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN经典算法——LeNet模型</title>
      <link href="2020/11/09/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94LeNet%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/"/>
      <url>2020/11/09/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94LeNet%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="LeNet模型介绍"><a href="#LeNet模型介绍" class="headerlink" title="LeNet模型介绍"></a>LeNet模型介绍</h1><img src="/2020/11/09/DL-CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94LeNet%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/1.jpg" class><p>LeNet网络基本架构为：conv1 (6) -&gt; pool1 -&gt; conv2 (16) -&gt; pool2 -&gt; fc3 (120) -&gt; fc4 (84) -&gt; fc5 (10) -&gt; softmax，括号内数字表示channel数。</p><p>该网络包含五层，两个卷积（卷积层、池化层)，两个全连接层，一个输出层。</p><p>其中卷积层的卷积核大小为5x5，stride=1；池化层为最大池化层，卷积核大小为2x2，strides=2；全连接层的输出个数分别为120， 84， 10.</p><h1 id="LeNet模型复现"><a href="#LeNet模型复现" class="headerlink" title="LeNet模型复现"></a>LeNet模型复现</h1><p>参考：《动手学深度学习》Chapter5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><h2 id="基于Sequntial类构造模型"><a href="#基于Sequntial类构造模型" class="headerlink" title="基于Sequntial类构造模型"></a>基于Sequntial类构造模型</h2><p>经历一次卷积层，其高和宽为(h-k+1)*(w-k+1)；经历一次池化层，其宽和高减半；这个过程重复两次</p><p>第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。</p><p>全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        </span><br><span class="line">        nn.Dense(<span class="number">120</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">        nn.Dense(<span class="number">84</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><h2 id="输入一个测试数据看看各层的输出形状"><a href="#输入一个测试数据看看各层的输出形状" class="headerlink" title="输入一个测试数据看看各层的输出形状"></a>输入一个测试数据看看各层的输出形状</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = nd.random.uniform(shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">net.initialize()</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    print(layer.name, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><pre><code>conv0 output shape:     (1, 6, 28, 28)pool0 output shape:     (1, 6, 14, 14)conv1 output shape:     (1, 16, 10, 10)pool1 output shape:     (1, 16, 5, 5)dense0 output shape:     (1, 120)dense1 output shape:     (1, 84)dense2 output shape:     (1, 10)</code></pre><h2 id="调用GPU计算"><a href="#调用GPU计算" class="headerlink" title="调用GPU计算"></a>调用GPU计算</h2><p>这里在安装的时候，不能直接用”pip install mxnet”,要用”pip install mxnet-cu100”cu100需要根据自己的cuda版本来更改<br>只有安装gpu版本的mxnet，才可使用gpu</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span>():</span> </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line">ctx = try_gpu()</span><br><span class="line">ctx</span><br></pre></td></tr></table></figure><pre><code>gpu(0)</code></pre><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><p>采用小批次训练，每个批次设置为256个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="评估准确度"><a href="#评估准确度" class="headerlink" title="评估准确度"></a>评估准确度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net, ctx</span>):</span></span><br><span class="line">    acc_sum, n = nd.array([<span class="number">0</span>], ctx=ctx), <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment">#将X，y复制到GPU中</span></span><br><span class="line">        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        acc_sum += (net(X).argmax(axis=<span class="number">1</span>) == y).sum()</span><br><span class="line">        n += y.size</span><br><span class="line">    <span class="keyword">return</span> acc_sum.asscalar() / n</span><br></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span>(<span class="params">net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;training on&#x27;</span>, ctx)</span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n, start = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            X, y = X.as_in_context(ctx), y.as_in_context(ctx)</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                y_hat = net(X)</span><br><span class="line">                l = loss(y_hat, y).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step(batch_size)</span><br><span class="line">            y = y.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            train_l_sum += l.asscalar()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(axis=<span class="number">1</span>) == y).sum().asscalar()</span><br><span class="line">            n += y.size</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net, ctx)</span><br><span class="line">        print(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, &#x27;</span></span><br><span class="line">              <span class="string">&#x27;time %.1f sec&#x27;</span></span><br><span class="line">              %(epoch+<span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc, time.time()-start))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.95</span>, <span class="number">35</span></span><br><span class="line">net.initialize(force_reinit=<span class="literal">True</span>, ctx=ctx, init=init.Xavier())</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">&#x27;sgd&#x27;</span>, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: lr&#125;)</span><br><span class="line">train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)</span><br></pre></td></tr></table></figure><pre><code>training on gpu(0)epoch 1, loss 2.3222, train acc 0.102, test acc 0.100, time 2.2 secepoch 2, loss 1.5564, train acc 0.396, test acc 0.650, time 2.2 secepoch 3, loss 0.8515, train acc 0.667, test acc 0.716, time 2.2 secepoch 4, loss 0.6984, train acc 0.722, test acc 0.749, time 2.2 secepoch 5, loss 0.6321, train acc 0.750, test acc 0.769, time 2.1 secepoch 6, loss 0.5789, train acc 0.771, test acc 0.793, time 2.2 secepoch 7, loss 0.5328, train acc 0.790, test acc 0.801, time 2.2 secepoch 8, loss 0.4997, train acc 0.805, test acc 0.820, time 2.2 secepoch 9, loss 0.4771, train acc 0.815, test acc 0.831, time 2.2 secepoch 10, loss 0.4515, train acc 0.829, test acc 0.840, time 2.2 secepoch 11, loss 0.4339, train acc 0.837, test acc 0.841, time 2.1 secepoch 12, loss 0.4220, train acc 0.841, test acc 0.852, time 2.2 secepoch 13, loss 0.4057, train acc 0.849, test acc 0.859, time 2.2 secepoch 14, loss 0.3916, train acc 0.854, test acc 0.856, time 2.2 secepoch 15, loss 0.3778, train acc 0.860, test acc 0.868, time 2.2 secepoch 16, loss 0.3675, train acc 0.865, test acc 0.873, time 2.3 secepoch 17, loss 0.3558, train acc 0.868, test acc 0.870, time 2.2 secepoch 18, loss 0.3472, train acc 0.873, test acc 0.868, time 2.2 secepoch 19, loss 0.3395, train acc 0.875, test acc 0.879, time 2.2 secepoch 20, loss 0.3335, train acc 0.878, test acc 0.873, time 2.2 secepoch 21, loss 0.3252, train acc 0.881, test acc 0.873, time 2.2 secepoch 22, loss 0.3194, train acc 0.882, test acc 0.883, time 2.2 secepoch 23, loss 0.3151, train acc 0.884, test acc 0.879, time 2.2 secepoch 24, loss 0.3124, train acc 0.884, test acc 0.882, time 2.2 secepoch 25, loss 0.3072, train acc 0.886, test acc 0.883, time 2.2 secepoch 26, loss 0.3032, train acc 0.888, test acc 0.885, time 2.2 secepoch 27, loss 0.2977, train acc 0.890, test acc 0.885, time 2.3 secepoch 28, loss 0.2956, train acc 0.891, test acc 0.880, time 2.2 secepoch 29, loss 0.2903, train acc 0.893, test acc 0.887, time 2.2 secepoch 30, loss 0.2902, train acc 0.893, test acc 0.890, time 2.1 secepoch 31, loss 0.2842, train acc 0.895, test acc 0.889, time 2.2 secepoch 32, loss 0.2807, train acc 0.897, test acc 0.890, time 2.2 secepoch 33, loss 0.2800, train acc 0.896, test acc 0.891, time 2.3 secepoch 34, loss 0.2745, train acc 0.899, test acc 0.892, time 2.2 secepoch 35, loss 0.2731, train acc 0.899, test acc 0.892, time 2.2 sec</code></pre><p>参考：<a href="https://blog.csdn.net/dan_teng/article/details/87192430">https://blog.csdn.net/dan_teng/article/details/87192430</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Paper Recurrence </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习（李宏毅2020spring）》作业3：CNN</title>
      <link href="2020/11/04/ML-my-hw3-cnn/"/>
      <url>2020/11/04/ML-my-hw3-cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="作业说明：CNN分类问题"><a href="#作业说明：CNN分类问题" class="headerlink" title="作业说明：CNN分类问题"></a>作业说明：CNN分类问题</h1><p>根据实物照片进行分类，照片的命名规则是“类别_编号.jpg”，类别也是用数字表示，共有11类，Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit；其实具体类别是什么我们可以不用管，只需要训练的时候输入数据，输出类别即可。<br>training 以及 validation 中的照片名名称格式為“类别_编号.jpg”，例如 3_100.jpg 即为类别 3 的照片（编号不重要）testing 中的照片名稱格式為 [编号].jpg<br>数据集需要提前下载下来，解压即可，范例中是从谷歌云下载的，需要翻墙，比较麻烦<br>参考：李宏毅机器学习第三次作业源码</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>cv2需要额外安装，直接输入命令pip install opencv-python即可完成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import需要的套件</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><h1 id="读入数据"><a href="#读入数据" class="headerlink" title="读入数据"></a>读入数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取文件的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readfile</span>(<span class="params">path, label</span>):</span></span><br><span class="line">    <span class="comment">#path为路径</span></span><br><span class="line">    <span class="comment">#label是一个布尔值，判断是否需要传回y的值，label=true的时候需要返回y的值，此时是训练过程；label=false的时候，不需要返回y的值，此时是测试过程</span></span><br><span class="line">    <span class="comment">#图片是128*128的RGB图片，用三维数组保存</span></span><br><span class="line">    image_dir = sorted(os.listdir(path))</span><br><span class="line">    x = np.zeros((len(image_dir), <span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>), dtype=np.uint8)</span><br><span class="line">    y = np.zeros((len(image_dir)), dtype=np.uint8)</span><br><span class="line">    <span class="keyword">for</span> i, file <span class="keyword">in</span> enumerate(image_dir):</span><br><span class="line">        img = cv2.imread(os.path.join(path, file))</span><br><span class="line">        x[i, :, :] = cv2.resize(img,(<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">        <span class="keyword">if</span> label:</span><br><span class="line">            <span class="comment">#此处返回的是类别</span></span><br><span class="line">            y[i] = int(file.split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> label:</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">workspace_dir = <span class="string">&#x27;./food-11&#x27;</span></span><br><span class="line">print(<span class="string">&quot;Reading data&quot;</span>)</span><br><span class="line">train_x, train_y = readfile(os.path.join(workspace_dir, <span class="string">&quot;training&quot;</span>), <span class="literal">True</span>)</span><br><span class="line">print(<span class="string">&quot;Size of training data = &#123;&#125;&quot;</span>.format(len(train_x)))</span><br><span class="line">val_x, val_y = readfile(os.path.join(workspace_dir, <span class="string">&quot;validation&quot;</span>), <span class="literal">True</span>)</span><br><span class="line">print(<span class="string">&quot;Size of validation data = &#123;&#125;&quot;</span>.format(len(val_x)))</span><br><span class="line">test_x = readfile(os.path.join(workspace_dir, <span class="string">&quot;testing&quot;</span>), <span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&quot;Size of Testing data = &#123;&#125;&quot;</span>.format(len(test_x)))</span><br></pre></td></tr></table></figure><pre><code>Reading dataSize of training data = 9866Size of validation data = 3430Size of Testing data = 3347</code></pre><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>在 Pytorch 中，我們可以利用 torch.utils.data 的 Dataset 及 DataLoader 來”包裝” data，使後續的 training 及 testing 更為方便</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#training时需要做数据增强</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),</span><br><span class="line">    transforms.RandomHorizontalFlip(), <span class="comment">#随机将图片水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">15</span>), <span class="comment">#随机旋转图片</span></span><br><span class="line">    transforms.ToTensor(), <span class="comment">#将图片反转成tensor</span></span><br><span class="line">])</span><br><span class="line"><span class="comment">#testing时不需要做数据增强</span></span><br><span class="line">test_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),                                    </span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y=None, transform=None</span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        <span class="comment"># label is required to be a LongTensor</span></span><br><span class="line">        self.y = y</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.y = torch.LongTensor(y)</span><br><span class="line">        self.transform = transform</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> len(self.x)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        X = self.x[index]</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            X = self.transform(X)</span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            Y = self.y[index]</span><br><span class="line">            <span class="keyword">return</span> X, Y</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>分批次训练，可以加快训练的速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_set = ImgDataset(train_x, train_y, train_transform)</span><br><span class="line">val_set = ImgDataset(val_x, val_y, test_transform)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h1 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h1><p>BatchNorm2d对数据进行归一化处理<br>激活函数为ReLu</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Classifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Classifier, self).__init__()</span><br><span class="line">        <span class="comment">#torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</span></span><br><span class="line">        <span class="comment">#torch.nn.MaxPool2d(kernel_size, stride, padding)</span></span><br><span class="line">        <span class="comment">#input 維度 [3, 128, 128]</span></span><br><span class="line">        self.cnn = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),  <span class="comment"># [64, 128, 128]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>),      <span class="comment"># [64, 64, 64]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [128, 64, 64]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>),      <span class="comment"># [128, 32, 32]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [256, 32, 32]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>),      <span class="comment"># [256, 16, 16]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [512, 16, 16]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>),       <span class="comment"># [512, 8, 8]</span></span><br><span class="line">            </span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [512, 8, 8]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>),       <span class="comment"># [512, 4, 4]</span></span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">11</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.cnn(x)</span><br><span class="line">        out = out.view(out.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.fc(out)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>使用trainingset训练，使用validationset寻找好的的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">model = Classifier().cuda()</span><br><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 因為是 classification task，所以 loss 使用 CrossEntropyLoss</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>) <span class="comment"># optimizer 使用 Adam.lr为超参数，可以修改</span></span><br><span class="line">num_epoch = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    model.train() <span class="comment"># 確保 model 是在 train model (開啟 Dropout 等...)</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 用 optimizer 將 model 參數的 gradient 歸零</span></span><br><span class="line">        train_pred = model(data[<span class="number">0</span>].cuda()) <span class="comment"># 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數</span></span><br><span class="line">        batch_loss = loss(train_pred, data[<span class="number">1</span>].cuda()) <span class="comment"># 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）</span></span><br><span class="line">        batch_loss.backward() <span class="comment"># 利用 back propagation 算出每個參數的 gradient</span></span><br><span class="line">        optimizer.step() <span class="comment"># 以 optimizer 用 gradient 更新參數值</span></span><br><span class="line"></span><br><span class="line">        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">        train_loss += batch_loss.item()</span><br><span class="line">    </span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">            val_pred = model(data[<span class="number">0</span>].cuda())</span><br><span class="line">            batch_loss = loss(val_pred, data[<span class="number">1</span>].cuda())</span><br><span class="line"></span><br><span class="line">            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">            val_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#將結果 print 出來</span></span><br><span class="line">        print(<span class="string">&#x27;[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f&#x27;</span> % \</span><br><span class="line">            (epoch + <span class="number">1</span>, num_epoch, time.time()-epoch_start_time, \</span><br><span class="line">             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))</span><br></pre></td></tr></table></figure><pre><code>[001/030] 14.86 sec(s) Train Acc: 0.227549 Loss: 0.018395 | Val Acc: 0.243440 loss: 0.016740[002/030] 14.73 sec(s) Train Acc: 0.340158 Loss: 0.014800 | Val Acc: 0.334111 loss: 0.016127[003/030] 14.75 sec(s) Train Acc: 0.381512 Loss: 0.013810 | Val Acc: 0.313994 loss: 0.015460[004/030] 14.77 sec(s) Train Acc: 0.431482 Loss: 0.012794 | Val Acc: 0.317493 loss: 0.015771[005/030] 14.83 sec(s) Train Acc: 0.467160 Loss: 0.012125 | Val Acc: 0.441691 loss: 0.013532[006/030] 14.82 sec(s) Train Acc: 0.501824 Loss: 0.011313 | Val Acc: 0.190962 loss: 0.029743[007/030] 14.82 sec(s) Train Acc: 0.530002 Loss: 0.010675 | Val Acc: 0.351603 loss: 0.018797[008/030] 14.82 sec(s) Train Acc: 0.550882 Loss: 0.010232 | Val Acc: 0.527697 loss: 0.010849[009/030] 14.80 sec(s) Train Acc: 0.573383 Loss: 0.009685 | Val Acc: 0.407872 loss: 0.014792[010/030] 14.84 sec(s) Train Acc: 0.582911 Loss: 0.009314 | Val Acc: 0.359767 loss: 0.019833[011/030] 14.86 sec(s) Train Acc: 0.617981 Loss: 0.008696 | Val Acc: 0.557434 loss: 0.011074[012/030] 14.84 sec(s) Train Acc: 0.625481 Loss: 0.008536 | Val Acc: 0.522449 loss: 0.011264[013/030] 14.86 sec(s) Train Acc: 0.657004 Loss: 0.007838 | Val Acc: 0.580175 loss: 0.010005[014/030] 14.80 sec(s) Train Acc: 0.674133 Loss: 0.007390 | Val Acc: 0.582799 loss: 0.009706[015/030] 14.86 sec(s) Train Acc: 0.686600 Loss: 0.007146 | Val Acc: 0.541108 loss: 0.011018[016/030] 14.83 sec(s) Train Acc: 0.696331 Loss: 0.006872 | Val Acc: 0.569971 loss: 0.010966[017/030] 14.88 sec(s) Train Acc: 0.716197 Loss: 0.006412 | Val Acc: 0.595335 loss: 0.010484[018/030] 14.88 sec(s) Train Acc: 0.719136 Loss: 0.006439 | Val Acc: 0.632945 loss: 0.009088[019/030] 14.94 sec(s) Train Acc: 0.727042 Loss: 0.006230 | Val Acc: 0.532653 loss: 0.012839[020/030] 14.90 sec(s) Train Acc: 0.736063 Loss: 0.005996 | Val Acc: 0.605831 loss: 0.010434[021/030] 14.85 sec(s) Train Acc: 0.757754 Loss: 0.005454 | Val Acc: 0.578717 loss: 0.011531[022/030] 14.88 sec(s) Train Acc: 0.766471 Loss: 0.005285 | Val Acc: 0.632070 loss: 0.009517[023/030] 14.88 sec(s) Train Acc: 0.784715 Loss: 0.004788 | Val Acc: 0.662391 loss: 0.008359[024/030] 14.83 sec(s) Train Acc: 0.804987 Loss: 0.004413 | Val Acc: 0.550729 loss: 0.013303[025/030] 14.85 sec(s) Train Acc: 0.813602 Loss: 0.004159 | Val Acc: 0.634985 loss: 0.010451[026/030] 14.85 sec(s) Train Acc: 0.821711 Loss: 0.004053 | Val Acc: 0.467055 loss: 0.017780[027/030] 14.93 sec(s) Train Acc: 0.828198 Loss: 0.003934 | Val Acc: 0.579592 loss: 0.013445[028/030] 14.85 sec(s) Train Acc: 0.823535 Loss: 0.004012 | Val Acc: 0.616910 loss: 0.011728[029/030] 14.90 sec(s) Train Acc: 0.832455 Loss: 0.003813 | Val Acc: 0.628863 loss: 0.010894[030/030] 14.83 sec(s) Train Acc: 0.837726 Loss: 0.003696 | Val Acc: 0.679300 loss: 0.009366</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_val_x = np.concatenate((train_x, val_x), axis=<span class="number">0</span>)</span><br><span class="line">train_val_y = np.concatenate((train_y, val_y), axis=<span class="number">0</span>)</span><br><span class="line">train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)</span><br><span class="line">train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">model_best = Classifier().cuda()</span><br><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 因為是 classification task，所以 loss 使用 CrossEntropyLoss</span></span><br><span class="line">optimizer = torch.optim.Adam(model_best.parameters(), lr=<span class="number">0.001</span>) <span class="comment"># optimizer 使用 Adam</span></span><br><span class="line">num_epoch = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    model_best.train()</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_val_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        train_pred = model_best(data[<span class="number">0</span>].cuda())</span><br><span class="line">        batch_loss = loss(train_pred, data[<span class="number">1</span>].cuda())</span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">        train_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#將結果 print 出來</span></span><br><span class="line">    print(<span class="string">&#x27;[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f&#x27;</span> % \</span><br><span class="line">      (epoch + <span class="number">1</span>, num_epoch, time.time()-epoch_start_time, \</span><br><span class="line">      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))</span><br></pre></td></tr></table></figure><pre><code>[001/030] 16.94 sec(s) Train Acc: 0.253234 Loss: 0.016887[002/030] 16.98 sec(s) Train Acc: 0.377708 Loss: 0.013877[003/030] 16.97 sec(s) Train Acc: 0.443442 Loss: 0.012409[004/030] 17.08 sec(s) Train Acc: 0.496916 Loss: 0.011322[005/030] 17.02 sec(s) Train Acc: 0.539486 Loss: 0.010247[006/030] 16.99 sec(s) Train Acc: 0.578896 Loss: 0.009436[007/030] 17.08 sec(s) Train Acc: 0.608303 Loss: 0.008864[008/030] 17.06 sec(s) Train Acc: 0.630039 Loss: 0.008328[009/030] 17.02 sec(s) Train Acc: 0.656965 Loss: 0.007704[010/030] 17.14 sec(s) Train Acc: 0.677873 Loss: 0.007283[011/030] 17.18 sec(s) Train Acc: 0.699158 Loss: 0.006727[012/030] 17.19 sec(s) Train Acc: 0.717735 Loss: 0.006340[013/030] 17.19 sec(s) Train Acc: 0.745262 Loss: 0.005799[014/030] 17.17 sec(s) Train Acc: 0.758574 Loss: 0.005436[015/030] 17.14 sec(s) Train Acc: 0.765569 Loss: 0.005278[016/030] 17.09 sec(s) Train Acc: 0.777828 Loss: 0.004947[017/030] 17.18 sec(s) Train Acc: 0.795427 Loss: 0.004533[018/030] 17.12 sec(s) Train Acc: 0.799188 Loss: 0.004513[019/030] 17.12 sec(s) Train Acc: 0.825887 Loss: 0.003908[020/030] 17.22 sec(s) Train Acc: 0.831980 Loss: 0.003822[021/030] 17.12 sec(s) Train Acc: 0.840177 Loss: 0.003572[022/030] 17.10 sec(s) Train Acc: 0.859582 Loss: 0.003178[023/030] 17.21 sec(s) Train Acc: 0.868758 Loss: 0.002964[024/030] 17.14 sec(s) Train Acc: 0.875150 Loss: 0.002801[025/030] 17.16 sec(s) Train Acc: 0.888613 Loss: 0.002458[026/030] 17.10 sec(s) Train Acc: 0.892449 Loss: 0.002349[027/030] 17.05 sec(s) Train Acc: 0.905836 Loss: 0.002034[028/030] 17.08 sec(s) Train Acc: 0.917043 Loss: 0.001871[029/030] 17.18 sec(s) Train Acc: 0.914786 Loss: 0.001894[030/030] 17.16 sec(s) Train Acc: 0.916291 Loss: 0.001863</code></pre><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_set = ImgDataset(test_x, transform=test_transform)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_best.eval()</span><br><span class="line">prediction = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(test_loader):</span><br><span class="line">        test_pred = model_best(data.cuda())</span><br><span class="line">        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> test_label:</span><br><span class="line">            prediction.append(y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">&quot;predict.csv&quot;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Id,Category\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span>  enumerate(prediction):</span><br><span class="line">        <span class="comment">#print(i, y)</span></span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.format(i, y))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>softmax回归模型</title>
      <link href="2020/10/31/DL-softmax%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/10/31/DL-softmax%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>线性回归输出结果为连续值，而对于分类问题，需要输出离散值，而softmax可以有效解决这一问题。</p></blockquote><h2 id="softmax回归从零开始实现"><a href="#softmax回归从零开始实现" class="headerlink" title="softmax回归从零开始实现"></a>softmax回归从零开始实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><h3 id="初始化参数模型"><a href="#初始化参数模型" class="headerlink" title="初始化参数模型"></a>初始化参数模型</h3><p>由于像素为28*28，因此输入为784，而输出为10个类别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, num_outputs))</span><br><span class="line">b = nd.zeros(num_outputs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#梯度</span></span><br><span class="line">W.attach_grad()</span><br><span class="line">b.attach_grad()</span><br></pre></td></tr></table></figure><h3 id="softmax运算实现"><a href="#softmax运算实现" class="headerlink" title="softmax运算实现"></a>softmax运算实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#看一下三维矩阵的运算</span></span><br><span class="line">X = nd.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">X.sum(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>), X.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>( [[5. 7. 9.]] &lt;NDArray 1x3 @cpu(0)&gt;, [[ 6.]  [15.]] &lt;NDArray 2x1 @cpu(0)&gt;)</code></pre><p>将每一行的值转换为概率，且行和为1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">X</span>):</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = nd.random.normal(shape=(<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">X_prob = softmax(X)</span><br><span class="line">X_prob, X_prob.sum(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>( [[0.21324193 0.33961776 0.1239742  0.27106097 0.05210521]  [0.11462264 0.3461234  0.19401033 0.29583326 0.04941036]] &lt;NDArray 2x5 @cpu(0)&gt;, [1.0000001 1.       ] &lt;NDArray 2 @cpu(0)&gt;)</code></pre><h3 id="定义模型、损失函数、分类准确率"><a href="#定义模型、损失函数、分类准确率" class="headerlink" title="定义模型、损失函数、分类准确率"></a>定义模型、损失函数、分类准确率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape(<span class="number">-1</span>, num_inputs), W) + b)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_hat = nd.array([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y = nd.array([<span class="number">0</span>, <span class="number">2</span>], dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">nd.pick(y_hat, y) </span><br></pre></td></tr></table></figure><pre><code>[0.1 0.5]&lt;NDArray 2 @cpu(0)&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> -nd.pick(y_hat, y).log()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(axis=<span class="number">1</span>) == y.astype(<span class="string">&#x27;float32&#x27;</span>)).mean().asscalar()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy(y_hat, y)</span><br></pre></td></tr></table></figure><pre><code>0.5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net</span>):</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        y = y.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        acc_sum += (net(X).argmax(axis=<span class="number">1</span>) == y).sum().asscalar()</span><br><span class="line">        n += y.size</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evaluate_accuracy(test_iter, net)</span><br></pre></td></tr></table></figure><pre><code>0.0925</code></pre><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr =<span class="number">10</span>, <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, trainer=None</span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                y_hat = net(X)</span><br><span class="line">                l = loss(y_hat, y).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> trainer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                trainer.step(batch_size)  <span class="comment"># “softmax回归的简洁实现”一节将用到</span></span><br><span class="line">            y = y.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            train_l_sum += l.asscalar()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(axis=<span class="number">1</span>) == y).sum().asscalar()</span><br><span class="line">            n += y.size</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size,</span><br><span class="line">          [W, b], lr)</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.3821, train acc 0.868, test acc 0.860epoch 2, loss 0.3823, train acc 0.868, test acc 0.857epoch 3, loss 0.3820, train acc 0.868, test acc 0.858epoch 4, loss 0.3819, train acc 0.869, test acc 0.858epoch 5, loss 0.3817, train acc 0.868, test acc 0.858epoch 6, loss 0.3817, train acc 0.868, test acc 0.857epoch 7, loss 0.3813, train acc 0.868, test acc 0.860epoch 8, loss 0.3813, train acc 0.868, test acc 0.858epoch 9, loss 0.3812, train acc 0.868, test acc 0.857epoch 10, loss 0.3813, train acc 0.868, test acc 0.859</code></pre><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>第一行为真实标签，第二行为预测标签，第三行为图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.asnumpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>).asnumpy())</span><br><span class="line">titles = [true + <span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]</span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure><img src="/2020/10/31/DL-softmax%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/1.png" class>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression线性回归模型</title>
      <link href="2020/10/30/DL-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/10/30/DL-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h1><p>参考：《动手学深度学习》<br>通过利用mxnet框架中的NDArray和autograd来实现一个线性回归的训练过程</p><h3 id="导入所需要的包"><a href="#导入所需要的包" class="headerlink" title="导入所需要的包"></a>导入所需要的包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>我们设定有两个特征值x1， x2，回归模型真实值为y = 2<em>x1 - 3.4</em>x2 + 4.2<br>其中w1 = 2, w2 = -3.4, b = 4.2<br>加入一个随机噪声∈来生成标签label，即y = 2<em>x1 - 3.4</em>x2 + 4.2 + ∈<br>∈符合均值为0，标准差为0.01的正态分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"><span class="comment">#随机生成特征值</span></span><br><span class="line">features = nd.random.normal(scale=<span class="number">1</span>, shape=(num_examples, num_inputs))</span><br><span class="line"><span class="comment">#根据特征值得到标签值</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += nd.random.normal(scale=<span class="number">0.01</span>, shape=labels.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">features[<span class="number">0</span>], labels[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>( [0.5086958  0.91511744] &lt;NDArray 2 @cpu(0)&gt;, [2.0881135] &lt;NDArray 1 @cpu(0)&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_svg_display</span>():</span></span><br><span class="line">    <span class="comment"># 用矢量图显示</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    <span class="comment"># 设置图的尺寸</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class="line"></span><br><span class="line">set_figsize()</span><br><span class="line"><span class="comment">#把features与label的对应关系展示出来</span></span><br><span class="line">plt.scatter(features[:, <span class="number">0</span>].asnumpy(), labels.asnumpy(), <span class="number">1</span>)</span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].asnumpy(), labels.asnumpy(), <span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PathCollection at 0x7fda87eb0c88&gt;</code></pre><img src="/2020/10/30/DL-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/1.png" class><p>​      </p><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从features中读取数据，我们每次读取batch_size个数据，</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment">#读取是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = nd.array(indices[i: min(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features.take(j), labels.take(j)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, y)</span><br><span class="line">    <span class="keyword">break</span> <span class="comment">#只输出一组batch_size，没有break将会输出所有的</span></span><br></pre></td></tr></table></figure><pre><code>[[ 0.01437022 -2.3105397 ] [-0.1742568   0.65264654] [ 1.2812718  -0.96987015] [-0.2819217  -0.24931197] [-0.11592119  0.0309534 ] [ 0.60624367  0.76104844] [-1.0258828   0.6976225 ] [-1.779809    0.5851231 ] [ 1.4180893  -0.62011564] [-0.87209934  0.6670822 ]]&lt;NDArray 10x2 @cpu(0)&gt; [12.096044    1.6319575  10.049137    4.503763    3.8513668   2.8237865 -0.23646423 -1.3559463   9.153078    0.19797151]&lt;NDArray 10 @cpu(0)&gt;</code></pre><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#权重初始化,权重初始化成均值为0，标准差为0.01的正太随机数</span></span><br><span class="line">w = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">b = nd.zeros(shape=(<span class="number">1</span>,))</span><br><span class="line"><span class="comment">#创建梯度</span></span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br></pre></td></tr></table></figure><h3 id="定义模型、损失函数、优化算法"><a href="#定义模型、损失函数、优化算法" class="headerlink" title="定义模型、损失函数、优化算法"></a>定义模型、损失函数、优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义线性模型模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span>(<span class="params">X, w, b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">params, lr, batch_size</span>):</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br></pre></td></tr></table></figure><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="comment">#其中学习率lr为0.03，迭代次数num_epochs</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs): </span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.000050epoch 2, loss 0.000050epoch 3, loss 0.000050epoch 4, loss 0.000050epoch 5, loss 0.000050</code></pre><h3 id="把学习得到的参数与真实参数对比"><a href="#把学习得到的参数与真实参数对比" class="headerlink" title="把学习得到的参数与真实参数对比"></a>把学习得到的参数与真实参数对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">true_w, w</span><br></pre></td></tr></table></figure><pre><code>([2, -3.4], [[ 2.0009615]  [-3.400259 ]] &lt;NDArray 2x1 @cpu(0)&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">true_b, b</span><br></pre></td></tr></table></figure><pre><code>(4.2, [4.2000113] &lt;NDArray 1 @cpu(0)&gt;)</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recurrent Neural Network循环神经网络</title>
      <link href="2020/10/29/DL-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>2020/10/29/DL-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1801.01078.pdf">RNN综述回顾</a></p><h1 id="Why-we-need-RNN"><a href="#Why-we-need-RNN" class="headerlink" title="Why we need  RNN?"></a>Why we need  RNN?</h1><p>对于全连接网络，层与层之间是全连接的，但是每层之间的节点是无连接的，因此全连接网络只能够处理单独的输入（即输入之间没有关联），前一个输入和后一个输入时完全没有关系的，无法处理相互之间关联的数据。</p><p>例如，我们输入两句话</p><blockquote><p>arrive BeiJing on November 2nd</p><p>leave BeiJing on November 2nd</p></blockquote><p>上面这两句话后四个单词都是一样的，但是前两个单词决定了这两句话的含义是完全不同的，即前一个单词对当前单词的含义有很大的影响，而全连接网络无法分辨出这种不同，因此我们需要Neral Network具备记忆功能，即在输入下一个单词之前记住前一个单词。</p><p>根据上面我们可以知道RNN用来处理遗传相互依赖的数据流，其用途十分广泛，例如文章中的文字、语音里的音频、股票的价格走势……</p><h1 id="The-Structural-of-RNN"><a href="#The-Structural-of-RNN" class="headerlink" title="The Structural of RNN"></a>The Structural of RNN</h1><img src="/2020/10/29/DL-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png" class><p>包括四个部分：输入层、隐藏层、循环层、输出层；</p><p>输入层经过权重求值之后，在隐藏层经过激活函数得到一个值，这个值进行权重求解之后到输出层的同时，也会保存到循环层存储起来，在下一次有输入的时候，循环层存储的值会再次与输入层的值一起经过激活函数到输出层。</p><p>我们举个例子来说明一下：</p><blockquote><p>假设所有的权重均为1，没有bias；所有的激活函数都是线性的；</p><p>输入队列分别是：[1 1] 、[1 1]、[2 2]</p><p>当第一次输入[x1 x2]=[1 1]时，隐藏层得到的结果是[2 2]，这个结果会被存储到循环层[a1 a2]=[2 2]，隐藏层进行加权求值，得到输出层结果[4 4]；</p><p>当第二次输入[x1 x2]=[1 1]时，隐藏层得到的结果是[6 6]，因为这个结果不仅考虑了输入值，还会考虑循环层的值，这个结果会被存储到循环层[a1 a2]=[6 6]，隐藏层进行加权求值，得到输出层结果[12 12]；</p><p>当第三次输入[x1 x2]=[2 2 ]时，隐藏层得到的结果是[16 16]，同样是因为这个结果考虑输入层和循环层，这个结果会被存储到循环层[a1 a2]=[16 16]，隐藏层进行加权求值，得到输出层结果[32 32 ]；</p><p>如果对上面的过程还是不理解，可以自己手动带入推演一下即可。</p></blockquote><p>上面讲解的只是一个层次，深度学习需要多个层次，因此我们需要把许多层连接起来</p><img src="/2020/10/29/DL-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png" class><h1 id="RNN优化算法（持续更新"><a href="#RNN优化算法（持续更新" class="headerlink" title="RNN优化算法（持续更新)"></a>RNN优化算法（持续更新)</h1><h2 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h2><p>参考</p><p><a href="https://www.jianshu.com/p/c0fa54dd20f7">https://www.jianshu.com/p/c0fa54dd20f7</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《动手学深度学习》笔记</title>
      <link href="2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
      <url>2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="相关资料："><a href="#相关资料：" class="headerlink" title="相关资料："></a>相关资料：</h1><p><a href="https://zh.d2l.ai/">书籍地址</a></p><p><a href="https://mxnet.apache.org/versions/1.7.0/">maxnet官网</a></p><h1 id="Chapter2：预备知识"><a href="#Chapter2：预备知识" class="headerlink" title="Chapter2：预备知识"></a>Chapter2：预备知识</h1><h2 id="获取和运行本书的代码"><a href="#获取和运行本书的代码" class="headerlink" title="获取和运行本书的代码"></a>获取和运行本书的代码</h2><p>环境配置按照官网教程即可实现，可能会遇到一个问题，我的cuda是10.0版本，在修改environment文件时要写成mxnet100，这样就可以运行了。</p><p>另外我是在GPU服务器上运行，为了可以在自己的浏览器打开jupyternotebook，需要设置一下，具体设置自行百度，唯一需要注意的是，修改文件后一定要把修改配置前的“#”删去，否则是无效的。</p><h1 id="Chapter3：深度学习基础"><a href="#Chapter3：深度学习基础" class="headerlink" title="Chapter3：深度学习基础"></a>Chapter3：深度学习基础</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>当模型和损失函数形式较为简单时，其误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。</p><p>大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）</p><p>对于求解数值解，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用，其做法就是选定一个初始点w0，然后w0沿着梯度下降到方向移动一定的距离，知道找到最小点。</p><p>线性回归模型实现，比较简单。</p><h2 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h2><p>这本书对softmax回归讲解还是比较详细，尤其是交叉熵这部分，要重点看一下，后面会经常用到。</p><p>线性回归的输出是连续的值，但是当我们需要做分类问题的时候，输出往往只有类别之分，是离散的值。softmax回归，其实是在线性回归的基础之上</p><h2 id="多层感知机（常用的激活函数）"><a href="#多层感知机（常用的激活函数）" class="headerlink" title="多层感知机（常用的激活函数）"></a>多层感知机（常用的激活函数）</h2><p>下面的输出均为-8，8之间随机生成的数值</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/1.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/2.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/3.png" class><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/4.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/5.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/6.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/7.png" class><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/8.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/9.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/10.png" class><img src="/2020/10/28/DL-%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/11.png" class><h2 id="模型选择、欠拟合、过拟合"><a href="#模型选择、欠拟合、过拟合" class="headerlink" title="模型选择、欠拟合、过拟合"></a>模型选择、欠拟合、过拟合</h2><p>训练误差：指模型在训练数据集上表现出的误差。</p><p>泛化误差：指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。</p><h3 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h3><p>预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p><h3 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>在k折交叉验证中，我们把原始训练数据集分割成kk个不重合的子数据集，然后我们做kk次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他k−1k−1个子数据集来训练模型。在这kk次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这kk次训练误差和验证误差分别求平均。</p><h3 id="欠拟合、过拟合"><a href="#欠拟合、过拟合" class="headerlink" title="欠拟合、过拟合"></a>欠拟合、过拟合</h3><p>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）</p><h1 id="Chapter5"><a href="#Chapter5" class="headerlink" title="Chapter5"></a>Chapter5</h1><h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><p><a href="https://blog.csdn.net/dan_teng/article/details/87192430">https://blog.csdn.net/dan_teng/article/details/87192430</a></p><h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p><a href="https://blog.csdn.net/Yasin0/article/details/93379629">https://blog.csdn.net/Yasin0/article/details/93379629</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN图神经网络</title>
      <link href="2020/10/28/DL-GNN%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>2020/10/28/DL-GNN%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Convolutional Neural Network卷积神经网络</title>
      <link href="2020/10/28/DL-CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>2020/10/28/DL-CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="The-whole-CNN"><a href="#The-whole-CNN" class="headerlink" title="The whole CNN"></a>The whole CNN</h1><img src="/2020/10/28/DL-CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.png" class><p>CNN工作原理：输入一张图片，让他经历一系列卷积层、池化层和全连接层，最终得到输出，输出结果可以是图像内容的单独分类或者分类的概率。</p><p>整个CNN包括三个层次：输入层、卷积层、全连接层</p><p>输入层：输入图片；</p><p>卷积层：一个卷积层包括卷积和池化两个步骤，构成一个单元，一张图片经过卷积层之后将会得到一张新的图片；可以经过多次卷积处理；</p><p>全连接层：Fully Connected Feedforward network</p><h1 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h1><img src="/2020/10/28/DL-CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.gif" class><h2 id><a href="#" class="headerlink" title></a></h2><p>卷积神经网络是含有卷积层的神经网络。我们以二维为例，包含有高H和宽W两个维度，通常用来处理图像数据。</p><h2 id="卷积核（滤波器filter）"><a href="#卷积核（滤波器filter）" class="headerlink" title="卷积核（滤波器filter）"></a>卷积核（滤波器filter）</h2><p>二维卷积层就是把二维输入数组与二维卷积核进行内积运算得到一个二维输出数组。</p><p><strong>卷积核又称为过滤器filter</strong>，其大小取决于卷积核的高和宽，通常大小应该是奇数1、3、5、7……。我们以下面这个图作为展示：</p><img src="/2020/10/28/DL-CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png" class><p>将输入数组的深色区域（卷积核窗口）与卷积核进行内积得到一个输出数组的深色区域数字，即0×0+1×1+3×2+4×3=19，然后从左到右、从上到下依次移动卷积核窗口并求内积即可得到输出数组。其中输入数组的深色区域大小取决于卷积核的大小，即两者同高同宽。</p><p>卷积核的选择决定对图像的处理效果，可以看作是特征标识符，将图像的某些特征（直边缘、原色、锐化等）表示出来，其他特征尽可能忽略掉。常用的有图像锐化滤波器（Sharpness Filter）、边缘检测滤波器（Edge Detection Filter）、浮雕滤波器（Embossing Filter)</p><h2 id="填充Padding和步幅stride"><a href="#填充Padding和步幅stride" class="headerlink" title="填充Padding和步幅stride"></a>填充Padding和步幅stride</h2><p>在对输入数组依次移动求内积的过程中，边界元素（某一行或某一列）往往无法单独进行内积运算，因此我们引入填充，填充是指在输入数组的高宽两侧填充元素（通常为0）。</p><p>步幅是指卷积核窗口在输入数组上每次移动的行数或列数；</p><p>填充可以增加输出的高和宽，通常用来使输出与输入具有相同的高和宽；</p><p>步幅可以减小输出的高和宽，步幅变大之后输出数组的维数就会降低；</p><h2 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h2><p>上面我们讲的都是二维输入数组，适用于纯色图片，但是不适用于彩色图片。彩色图片是由RGB三种颜色叠加形成，相当于是三个二维数组，我们称为三个通道Chanel。</p><p>多输入通道对应的卷积核也是多通道，输出也是多通道。</p><h1 id="池化层Pooling（也成为下采样层down-sampling）"><a href="#池化层Pooling（也成为下采样层down-sampling）" class="headerlink" title="池化层Pooling（也成为下采样层down sampling）"></a>池化层Pooling（也成为下采样层down sampling）</h1><p>池化层每次对输入数据的一个固定形状窗口（称为池化窗口）中的元素进行计算输出。卷积层是把输入层与卷积核求内积，而池化层则是计算池化窗口内元素的最大值Max Pooling或平均值。</p><p>池化层可以压缩数据，减少参数数量。</p><p>填充和步幅都同上。</p><p>参考：</p><p><a href="http://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html">http://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html</a></p><p><a href="https://blog.csdn.net/zouxy09/article/details/49080029">https://blog.csdn.net/zouxy09/article/details/49080029</a></p><p>《机器学习（李宏毅2020spring）》</p><blockquote><p>2020年11月9日更新</p><p>多层感知机模型：有一定的局限性。</p><ol><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。假设输入是高和宽均为1,0001,000像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状也是3,000,000×2563,000,000×256：它占用了大约3 GB的内存或显存。这会带来过于复杂的模型和过高的存储开销。</li></ol><p>卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习资源汇总</title>
      <link href="2020/10/27/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/"/>
      <url>2020/10/27/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="论文网站"><a href="#论文网站" class="headerlink" title="论文网站"></a>论文网站</h1><p><a href="https://dblp.uni-trier.de/">DBLP</a>：计算机领域内对研究的成果以作者为核心的一个计算机类英文文献的继承数据库系统。</p><p><a href="https://arxiv.org/">arXiv</a>：（<em>X</em> 依<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E5%B8%8C%E8%87%98%E6%96%87">希腊文</a>的 <a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%CE%A7">χ</a> 发音，读音如英语的 <a href="https://link.zhihu.com/?target=https://zh.wiktionary.org/wiki/archive">archive</a> ）是一个收集<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E7%89%A9%E7%90%86%E5%AD%B8">物理学</a>、<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E6%95%B8%E5%AD%B8">数学</a>、<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E8%A8%88%E7%AE%97%E6%A9%9F%E7%A7%91%E5%AD%B8">计算机科学、</a><a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E7%94%9F%E7%89%A9%E5%AD%B8">生物学</a>与<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E6%95%B0%E7%90%86%E7%BB%8F%E6%B5%8E%E5%AD%A6">数理经济学</a>的<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E8%AB%96%E6%96%87">论文</a><a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E9%A0%90%E5%8D%B0%E6%9C%AC">预印本</a>的网站，始于1991年8月14日。截至2008年10月，arXiv.org已收集超过50万篇预印本[<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/ArXiv%23cite_note-2">2]</a>[<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/ArXiv%23cite_note-3">3]</a>；至2014年底，藏量达到1百万篇。截至2016年10月，提交率已达每月超过10,000篇。</p><p><a href="https://www.ieee.org/">IEEE</a></p><p>LaTex在线编辑器：<a href="http://www.overleaf.com/">www.overleaf.com</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习（李宏毅2020spring）》作业2：Classification</title>
      <link href="2020/10/21/ML-my-hw2-classification/"/>
      <url>2020/10/21/ML-my-hw2-classification/</url>
      
        <content type="html"><![CDATA[<h1 id="作业说明"><a href="#作业说明" class="headerlink" title="作业说明"></a>作业说明</h1><h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><p>根据人们的个人资料，判断其年收入是否高于5000美元，典型的二元分类问题，通过logistic regression与generative model实现</p><h2 id="数据集说明"><a href="#数据集说明" class="headerlink" title="数据集说明"></a>数据集说明</h2><p>X_train、Y_train和X_test是经过处理的数据集，可以直接使用，其他两个train.csv和test.csv为了提供额外信息</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/HL-space/p/10785225.html">https://www.cnblogs.com/HL-space/p/10785225.html</a><br><a href="https://mrsuncodes.github.io/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/">https://mrsuncodes.github.io/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/</a></p><p>李宏毅老师提供的源代码</p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>对每个属性做归一化，处理后将其分为训练集与测试集</p><p><a href="https://blog.csdn.net/pipisorry/article/details/52247379">https://blog.csdn.net/pipisorry/article/details/52247379</a><br>这篇文章对数据的归一化处理做了讲解，可以去这里看看，集中归一化方法都用到了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据，路径要和自己的路径保持一致</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X_train_fpath = <span class="string">&#x27;./X_train2&#x27;</span></span><br><span class="line">Y_train_fpath = <span class="string">&#x27;./Y_train2&#x27;</span></span><br><span class="line">X_test_fpath = <span class="string">&#x27;./X_test2&#x27;</span></span><br><span class="line">output_fpath = <span class="string">&#x27;./output_&#123;&#125;.csv&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将csv文件转换为numpy array</span></span><br><span class="line"><span class="comment">#源文件是通过‘，’作为分隔，每个值都有，无空值</span></span><br><span class="line"><span class="keyword">with</span> open(X_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_train = np.array([line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype = float)</span><br><span class="line"><span class="keyword">with</span> open(Y_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f)</span><br><span class="line">    Y_train = np.array([line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype = float)</span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_test = np.array([line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype = float)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalize</span>(<span class="params">X, train = True, specified_column = None, X_mean = None, X_std = None</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对X的特定列进行归一化处理</span></span><br><span class="line"><span class="string">    处理数据的过程中，训练数据的平均值和标准差会被重复使用</span></span><br><span class="line"><span class="string">    input：</span></span><br><span class="line"><span class="string">    X：要被处理的数据集</span></span><br><span class="line"><span class="string">    train：处理训练数据时为true，处理测试数据时为false</span></span><br><span class="line"><span class="string">    specified_column：特定列将会进行归一化处理，如果为None，所有列都会进行归一化处理</span></span><br><span class="line"><span class="string">    X_mean：训练数据的平均值</span></span><br><span class="line"><span class="string">    X_std：标准差</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">    X 、X_mean、X_std</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> specified_column == <span class="literal">None</span>:</span><br><span class="line">        specified_column = np.arange(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X[:, specified_column] ,<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        X_std  = np.std(X[:, specified_column], <span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + <span class="number">1e-8</span>)</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span>(<span class="params">X, Y, dev_ratio = <span class="number">0.25</span></span>):</span></span><br><span class="line">    <span class="comment"># 将数据划分为training set and development set.</span></span><br><span class="line">    train_size = int(len(X) * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据进行归一化处理</span></span><br><span class="line">X_train, X_mean, X_std = _normalize(X_train, train = <span class="literal">True</span>)</span><br><span class="line">X_test, _, _= _normalize(X_test, train = <span class="literal">False</span>, specified_column = <span class="literal">None</span>, X_mean = X_mean, X_std = X_std)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#数据划分为training set and development set</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line">print(<span class="string">&#x27;Size of training set: &#123;&#125;&#x27;</span>.format(train_size))</span><br><span class="line">print(<span class="string">&#x27;Size of development set: &#123;&#125;&#x27;</span>.format(dev_size))</span><br><span class="line">print(<span class="string">&#x27;Size of testing set: &#123;&#125;&#x27;</span>.format(test_size))</span><br><span class="line">print(<span class="string">&#x27;Dimension of data: &#123;&#125;&#x27;</span>.format(data_dim))</span><br></pre></td></tr></table></figure><pre><code>Size of training set: 48830Size of development set: 5426Size of testing set: 27622Dimension of data: 510</code></pre><h1 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h1><p>这个几个函数在后面会重复使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    把数据的行打乱，其中X， Y为等长度数据（第一维相等）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> (X[randomize], Y[randomize])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    sigmoid函数的功能是求概率，为了防止溢出，设置了最大最小输出值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - (<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span>(<span class="params">X, w, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    这个是逻辑回归函数f(w, b) = sigmoid(wi*xi + b)</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    X:数据集；W：为权重；b:为误差</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#matmul为矩阵的乘法操作</span></span><br><span class="line">    <span class="keyword">return</span> _sigmoid(np.matmul(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span>(<span class="params">X, w, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过调用逻辑回归函数对X的每一行返回一个真正的预测值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#round将f的结果四舍五入，astype规定其为int类型，即最终返回的是0或1</span></span><br><span class="line">    <span class="keyword">return</span> np.round(_f(X, w, b)).astype(np.int)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span>(<span class="params">Y_pred, Y_label</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算预测的准确性，如果预测正确，返回值为0，否则返回值为1</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span>(<span class="params">y_pred, Y_label</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    求交叉熵</span></span><br><span class="line"><span class="string">    input：</span></span><br><span class="line"><span class="string">    y_pred：预测的可能性，为float类型向量</span></span><br><span class="line"><span class="string">    Y_label： 真正的标签值，为bool类型向量</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    output：</span></span><br><span class="line"><span class="string">    交叉熵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label), np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span>(<span class="params">X, Y_label, w, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    这个功能是计算交叉熵损失函数的梯度，对w，b求导</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = -np.sum(pred_error * X.T, <span class="number">1</span>)</span><br><span class="line">    b_grad = -np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, b_grad</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对w，b初始化为0</span></span><br><span class="line">w = np.zeros((data_dim,)) </span><br><span class="line">b = np.zeros((<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment">#对一些参数进行训练  </span></span><br><span class="line"><span class="comment">#可以进行调整这几个参数，分别为迭代次数、小批次包含的数据个数、学习率</span></span><br><span class="line">max_iter = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">15</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把每次的损失和准确度都通过图表展示出来</span></span><br><span class="line"><span class="comment">#分别为训练集损失、验证集损失、训练集正确率、验证集正确率</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算参数更新的次数</span></span><br><span class="line">step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重复多轮训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(max_iter):</span><br><span class="line">    <span class="comment"># 每轮开始之前打乱数据</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 最小批次的训练</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[idx*batch_size:(idx+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[idx*batch_size:(idx+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#梯度更新，学习率也随之改变，这个学习率有点简单，直接用学习率除以更新次数的跟</span></span><br><span class="line">        w = w - learning_rate/np.sqrt(step) * w_grad</span><br><span class="line">        b = b - learning_rate/np.sqrt(step) * b_grad</span><br><span class="line"></span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">#计算训练集和验证集</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)<span class="comment">#此时为float类型</span></span><br><span class="line">    Y_train_pred = np.round(y_train_pred)<span class="comment">#转换为bool类型</span></span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size)</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b) </span><br><span class="line">    Y_dev_pred = np.round(y_dev_pred) </span><br><span class="line">    dev_acc.append(_accuracy(Y_dev_pred, Y_dev))</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Training loss: &#123;&#125;&#x27;</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">&#x27;Development loss: &#123;&#125;&#x27;</span>.format(dev_loss[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">&#x27;Training accuracy: &#123;&#125;&#x27;</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">&#x27;Development accuracy: &#123;&#125;&#x27;</span>.format(dev_acc[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure><pre><code>Training loss: 0.265942625291867Development loss: 0.2854362130041641Training accuracy: 0.8858693426172435Development accuracy: 0.8781791374861777</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失曲线</span></span><br><span class="line">plt.plot(train_loss)</span><br><span class="line">plt.plot(dev_loss)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>])</span><br><span class="line">plt.savefig(<span class="string">&#x27;loss.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准确率曲线</span></span><br><span class="line">plt.plot(train_acc)</span><br><span class="line">plt.plot(dev_acc)</span><br><span class="line">plt.title(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>])</span><br><span class="line">plt.savefig(<span class="string">&#x27;acc.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2020/10/21/ML-my-hw2-classification/output_8_0.png" class><img src="/2020/10/21/ML-my-hw2-classification/output_8_0.png" class><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><p>把test的数据进行预测得到预测结果<br>结果保存在output2_logistic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath.format(<span class="string">&#x27;logistic&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;id,label\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span>  enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the most significant weights</span></span><br><span class="line"><span class="comment"># 找到权重中最大的前十项，</span></span><br><span class="line">ind = np.argsort(np.abs(w))[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">features = np.array(content)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">    print(features[i], w[i])</span><br></pre></td></tr></table></figure><pre><code> Unemployed full-time 1.1225676433808978 Not in universe -1.0573592082887484 Other Rel &lt;18 never married RP of subfamily -0.912973403518828 Child 18+ ever marr Not in a subfamily -0.8705099085602619 1 0.7950300190669547 Spouse of householder -0.750112419980567 Other Rel &lt;18 ever marr RP of subfamily -0.7491036728017868 Italy -0.7240219980088511capital losses 0.6611812623046104id 0.5751429906332644</code></pre><h1 id="第二种方法：generative-model"><a href="#第二种方法：generative-model" class="headerlink" title="第二种方法：generative model"></a>第二种方法：generative model</h1><p>训练集与测试集的处理方法与逻辑回归一样，因为generativemodel有可解析的最佳解，因此不必使用到验证集</p><h2 id="数据处理与归一化"><a href="#数据处理与归一化" class="headerlink" title="数据处理与归一化"></a>数据处理与归一化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parse csv files to numpy array</span></span><br><span class="line"><span class="keyword">with</span> open(X_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_train = np.array([line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype = float)</span><br><span class="line"><span class="keyword">with</span> open(Y_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f)</span><br><span class="line">    Y_train = np.array([line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype = float)</span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_test = np.array([line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype = float)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize training and testing data</span></span><br><span class="line">X_train, X_mean, X_std = _normalize(X_train, train = <span class="literal">True</span>)</span><br><span class="line">X_test, _, _= _normalize(X_test, train = <span class="literal">False</span>, specified_column = <span class="literal">None</span>, X_mean = X_mean, X_std = X_std)</span><br></pre></td></tr></table></figure><h2 id="求两个类别的平均值和协方差"><a href="#求两个类别的平均值和协方差" class="headerlink" title="求两个类别的平均值和协方差"></a>求两个类别的平均值和协方差</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute in-class mean</span></span><br><span class="line"><span class="comment">#将数据的两个类别分开</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mean_0 = np.mean(X_train_0, axis = <span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis = <span class="number">0</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros((data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros((data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shared covariance is taken as a weighted average of individual in-class covariance.</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train_0.shape[<span class="number">0</span>] + X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute inverse of covariance matrix.</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and accurately.</span></span><br><span class="line">u, s, v = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">inv = np.matmul(v.T * <span class="number">1</span> / s, u.T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directly compute weights and bias</span></span><br><span class="line">w = np.dot(inv, mean_0 - mean_1)</span><br><span class="line">b =  (<span class="number">-0.5</span>) * np.dot(mean_0, np.dot(inv, mean_0)) + <span class="number">0.5</span> * np.dot(mean_1, np.dot(inv, mean_1))\</span><br><span class="line">    + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute accuracy on training set</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line">print(<span class="string">&#x27;Training accuracy: &#123;&#125;&#x27;</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br></pre></td></tr></table></figure><pre><code>Training accuracy: 0.873820406959599</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict testing labels</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath.format(<span class="string">&#x27;generative&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;id,label\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span>  enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the most significant weights</span></span><br><span class="line">ind = np.argsort(np.abs(w))[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">features = np.array(content)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">    print(features[i], w[i])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Agriculture <span class="number">7.5625</span></span><br><span class="line"> <span class="number">41</span> <span class="number">-7.5</span></span><br><span class="line"> Retail trade <span class="number">6.828125</span></span><br><span class="line"> Forestry <span class="keyword">and</span> fisheries <span class="number">6.03125</span></span><br><span class="line"> <span class="number">29</span> <span class="number">-6.0</span></span><br><span class="line"> <span class="number">35</span> <span class="number">5.265625</span></span><br><span class="line"> <span class="number">34</span> <span class="number">-5.15625</span></span><br><span class="line"> Sales <span class="number">-5.1171875</span></span><br><span class="line"> Construction <span class="number">-5.111328125</span></span><br><span class="line"> <span class="number">37</span> <span class="number">-4.79296875</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习（李宏毅2020spring）》作业1：Regression</title>
      <link href="2020/10/19/ML-my-hw1-regression/"/>
      <url>2020/10/19/ML-my-hw1-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="作业要求、参考文献"><a href="#作业要求、参考文献" class="headerlink" title="作业要求、参考文献"></a>作业要求、参考文献</h1><h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><p>根据前9个小时的18个features（包含PM2.5）预测第十个小时的PM2.5</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html</a><br>基本是按照参考代码来的，加入了一些注释更方便理解</p><p>另外所有的print都可以去掉，加上只是为了检验是否输出正确的数据</p><h1 id="加载训练集数据"><a href="#加载训练集数据" class="headerlink" title="加载训练集数据"></a>加载训练集数据</h1><p>train.csv 的資料為 12 個月中，每個月取 20 天，每天 24 小時的資料(每小時資料有 18 個 features)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;./hw1_train.csv&#x27;</span>, encoding = <span class="string">&#x27;big5&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h1><p>取出需要的数值部分，即从第四列开始取数据<br>把输出的数据与train.csv对比即可发现不同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = data.iloc[:, <span class="number">3</span>:]</span><br><span class="line">data[data == <span class="string">&#x27;NR&#x27;</span>] = <span class="number">0</span></span><br><span class="line">raw_data = data.to_numpy()</span><br></pre></td></tr></table></figure><h1 id="提取特征值1"><a href="#提取特征值1" class="headerlink" title="提取特征值1"></a>提取特征值1</h1><p>将数据转置，即将原始4230×18按照每个月分组为12个月中的18个features×480Hours</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">month_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    sample = np.empty([<span class="number">18</span>, <span class="number">480</span>])</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        sample[:, day * <span class="number">24</span> : (day + <span class="number">1</span>) * <span class="number">24</span>] = raw_data[<span class="number">18</span> * (<span class="number">20</span> * month + day) : <span class="number">18</span> * (<span class="number">20</span> * month + day + <span class="number">1</span>), :]</span><br><span class="line">    month_data[month] = sample</span><br><span class="line">    </span><br><span class="line">print(month_data)</span><br></pre></td></tr></table></figure><h1 id="提取特征2"><a href="#提取特征2" class="headerlink" title="提取特征2"></a>提取特征2</h1><p>每个月有20*24=480h，每9个小时形成一个data，每个月就会有471个data，总资料数目是471×12笔，每笔数据中有9×18个features；<br>对应的target（第10个小时的PM2.5）为471 × 12</p><p><em>注意：471是怎么得到的？首先每个月有480个小时，只需要9个小时形成一组来预测第是个小时；举例来看：1-9是一组，预测10；2-10是一组来预测11；以此类推，可以得到471-479是最后一组，预测480</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = np.empty([<span class="number">12</span> * <span class="number">471</span>, <span class="number">18</span> * <span class="number">9</span>], dtype = float)</span><br><span class="line">y = np.empty([<span class="number">12</span> * <span class="number">471</span>, <span class="number">1</span>], dtype = float)</span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        <span class="keyword">for</span> hour <span class="keyword">in</span> range(<span class="number">24</span>):</span><br><span class="line">            <span class="keyword">if</span> day == <span class="number">19</span> <span class="keyword">and</span> hour &gt; <span class="number">14</span>:</span><br><span class="line">                <span class="comment">#执行下个月的数据</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            x[month * <span class="number">471</span> + day * <span class="number">24</span> + hour, :] = month_data[month][:,day * <span class="number">24</span> + hour : day * <span class="number">24</span> + hour + <span class="number">9</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>) <span class="comment">#vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)</span></span><br><span class="line">            y[month * <span class="number">471</span> + day * <span class="number">24</span> + hour, <span class="number">0</span>] = month_data[month][<span class="number">9</span>, day * <span class="number">24</span> + hour + <span class="number">9</span>] <span class="comment">#value</span></span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>[[14.  14.  14.  ...  2.   2.   0.5] [14.  14.  13.  ...  2.   0.5  0.3] [14.  13.  12.  ...  0.5  0.3  0.8] ... [17.  18.  19.  ...  1.1  1.4  1.3] [18.  19.  18.  ...  1.4  1.3  1.6] [19.  18.  17.  ...  1.3  1.6  1.8]][[30.] [41.] [44.] ... [17.] [24.] [29.]]</code></pre><h1 id="标准化Normalize"><a href="#标准化Normalize" class="headerlink" title="标准化Normalize"></a>标准化Normalize</h1><p>标准化方法有好几种，可参考下面找个博客，本文的标准化方法是Z-score方法<br><a href="https://www.cnblogs.com/lvdongjie/p/11349701.html">https://www.cnblogs.com/lvdongjie/p/11349701.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean_x = np.mean(x, axis = <span class="number">0</span>) <span class="comment">#18 * 9 </span></span><br><span class="line">std_x = np.std(x, axis = <span class="number">0</span>) <span class="comment">#18 * 9 </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)): <span class="comment">#12 * 471</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x[<span class="number">0</span>])): <span class="comment">#18 * 9 </span></span><br><span class="line">        <span class="keyword">if</span> std_x[j] != <span class="number">0</span>:</span><br><span class="line">            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>array([[-1.35825331, -1.35883937, -1.359222  , ...,  0.26650729,         0.2656797 , -1.14082131],       [-1.35825331, -1.35883937, -1.51819928, ...,  0.26650729,        -1.13963133, -1.32832904],       [-1.35825331, -1.51789368, -1.67717656, ..., -1.13923451,        -1.32700613, -0.85955971],       ...,       [-0.88092053, -0.72262212, -0.56433559, ..., -0.57693779,        -0.29644471, -0.39079039],       [-0.7218096 , -0.56356781, -0.72331287, ..., -0.29578943,        -0.39013211, -0.1095288 ],       [-0.56269867, -0.72262212, -0.88229015, ..., -0.38950555,        -0.10906991,  0.07797893]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#划分训练集和测试集，train_set 用来训练， validation_set用来验证结果</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">x_train_set = x[: math.floor(len(x) * <span class="number">0.8</span>), :]</span><br><span class="line">y_train_set = y[: math.floor(len(y) * <span class="number">0.8</span>), :]</span><br><span class="line">x_validation = x[math.floor(len(x) * <span class="number">0.8</span>):, :]</span><br><span class="line">y_validation = y[math.floor(len(y) * <span class="number">0.8</span>):, :]</span><br><span class="line">print(x_train_set)</span><br><span class="line">print(y_train_set)</span><br><span class="line">print(x_validation)</span><br><span class="line">print(y_validation)</span><br><span class="line">print(len(x_train_set))</span><br><span class="line">print(len(y_train_set))</span><br><span class="line">print(len(x_validation))</span><br><span class="line">print(len(y_validation))</span><br></pre></td></tr></table></figure><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">18</span> * <span class="number">9</span> + <span class="number">1</span></span><br><span class="line">w = np.zeros([dim, <span class="number">1</span>])</span><br><span class="line">x = np.concatenate((np.ones([<span class="number">12</span> * <span class="number">471</span>, <span class="number">1</span>]), x), axis = <span class="number">1</span>).astype(float)</span><br><span class="line">learning_rate = <span class="number">100</span></span><br><span class="line">iter_time = <span class="number">1000</span></span><br><span class="line">adagrad = np.zeros([dim, <span class="number">1</span>])</span><br><span class="line">eps = <span class="number">0.0000000001</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(iter_time):</span><br><span class="line">    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, <span class="number">2</span>))/<span class="number">471</span>/<span class="number">12</span>)<span class="comment">#rmse</span></span><br><span class="line">    <span class="keyword">if</span>(t%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        print(str(t) + <span class="string">&quot;:&quot;</span> + str(loss))</span><br><span class="line">    gradient = <span class="number">2</span> * np.dot(x.transpose(), np.dot(x, w) - y) <span class="comment">#dim*1</span></span><br><span class="line">    adagrad += gradient ** <span class="number">2</span></span><br><span class="line">    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)</span><br><span class="line">np.save(<span class="string">&#x27;myweight.npy&#x27;</span>, w)</span><br><span class="line">w</span><br></pre></td></tr></table></figure><pre><code>0:27.071214829194115100:33.78905859777455200:19.913751298197102300:13.531068193689693400:10.64546615844617500:9.277353455475062600:8.518042045956497700:8.014061987588418800:7.636756824775688900:7.336563740371121</code></pre><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">testdata = pd.read_csv(<span class="string">&#x27;./hw1_test.csv&#x27;</span>, header = <span class="literal">None</span>, encoding = <span class="string">&#x27;big5&#x27;</span>)</span><br><span class="line">test_data = testdata.iloc[:, <span class="number">2</span>:]</span><br><span class="line">test_data[test_data == <span class="string">&#x27;NR&#x27;</span>] = <span class="number">0</span></span><br><span class="line">test_data = test_data.to_numpy()</span><br><span class="line">test_x = np.empty([<span class="number">240</span>, <span class="number">18</span>*<span class="number">9</span>], dtype = float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">    test_x[i, :] = test_data[<span class="number">18</span> * i: <span class="number">18</span>* (i + <span class="number">1</span>), :].reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_x)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(test_x[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> std_x[j] != <span class="number">0</span>:</span><br><span class="line">            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]</span><br><span class="line">test_x = np.concatenate((np.ones([<span class="number">240</span>, <span class="number">1</span>]), test_x), axis = <span class="number">1</span>).astype(float)</span><br><span class="line">test_x</span><br></pre></td></tr></table></figure><pre><code>array([[ 1.        , -0.24447681, -0.24545919, ..., -0.67065391,        -1.04594393,  0.07797893],       [ 1.        , -1.35825331, -1.51789368, ...,  0.17279117,        -0.10906991, -0.48454426],       [ 1.        ,  1.5057434 ,  1.34508393, ..., -1.32666675,        -1.04594393, -0.57829812],       ...,       [ 1.        ,  0.3919669 ,  0.54981237, ...,  0.26650729,        -0.20275731,  1.20302531],       [ 1.        , -1.8355861 , -1.8360023 , ..., -1.04551839,        -1.13963133, -1.14082131],       [ 1.        , -1.35825331, -1.35883937, ...,  2.98427476,         3.26367657,  1.76554849]])</code></pre><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = np.load(<span class="string">&#x27;myweight.npy&#x27;</span>)</span><br><span class="line">ans_y = np.dot(test_x, w)</span><br><span class="line">ans_y</span><br></pre></td></tr></table></figure><h1 id="保存结果到csv"><a href="#保存结果到csv" class="headerlink" title="保存结果到csv"></a>保存结果到csv</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">&#x27;my_submit.csv&#x27;</span>, mode=<span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> submit_file:</span><br><span class="line">    csv_writer = csv.writer(submit_file)</span><br><span class="line">    header = [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">    print(header)</span><br><span class="line">    csv_writer.writerow(header)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">        row = [<span class="string">&#x27;id_&#x27;</span> + str(i), ans_y[i][<span class="number">0</span>]]</span><br><span class="line">        csv_writer.writerow(row)</span><br><span class="line">        print(row)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习（李宏毅2020spring）》学习笔记(持续更新)</title>
      <link href="2020/10/15/ML-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E6%9D%8E%E5%AE%8F%E6%AF%85%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>2020/10/15/ML-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E6%9D%8E%E5%AE%8F%E6%AF%85%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h1><p>李宏毅老师：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html">个人主页</a>在这里有上课的课件、课后题目数据集及源码</p><p>课程视频（B站）：<a href="https://www.bilibili.com/video/BV1JE411g7XF?from=search&seid=2314636143203069326">B站</a></p><p>课程笔记：<a href="https://datawhalechina.github.io/leeml-notes/#/">GitHub1</a>、<a href="https://github.com/Sakura-gh/ML-notes">GitHub2</a></p><p>作业说明及范例：<a href="https://github.com/Iallen520/lhy_DL_Hw">GitHub</a></p><p><a href="http://zh.d2l.ai/index.html">动手学深度学习</a>：这本书也不错，理论与实践结合紧密</p><p><a href="https://i.am.ai/roadmap/#machine-learning-roadmap">深度学习路线图</a></p><p><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a></p><p><a href="https://blog.csdn.net/u012589040/article/details/106206592?biz_id=102&utm_term=CNN%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-106206592&spm=1018.2118.3001.4187">CNN十大经典论文</a></p><p>TensorFlow：<a href="http://c.biancheng.net/view/1911.html">http://c.biancheng.net/view/1911.html</a></p><p>这个也可以参考一下：<a href="https://blog.csdn.net/iteapoy/article/details/105382315">https://blog.csdn.net/iteapoy/article/details/105382315</a></p><h1 id="Learning-Map"><a href="#Learning-Map" class="headerlink" title="Learning Map"></a>Learning Map</h1><img src="/2020/10/15/ML-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E6%9D%8E%E5%AE%8F%E6%AF%85%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" class title="LearningMap"><h1 id="HomeWork"><a href="#HomeWork" class="headerlink" title="HomeWork"></a>HomeWork</h1><table><thead><tr><th align="center">序号</th><th align="left">任务</th><th align="left">完成情况</th><th align="center">完成时间</th></tr></thead><tbody><tr><td align="center">1</td><td align="left">Linear Regression</td><td align="left"><a href="https://housanduo123.github.io/2020/10/19/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%852020spring%EF%BC%89%E4%BD%9C%E4%B8%9A1%EF%BC%9ARegression/">✔Regression</a></td><td align="center">2020/10/19</td></tr><tr><td align="center">2</td><td align="left">Classification</td><td align="left"><a href="https://housanduo123.github.io/2020/10/21/my-hw2-classification/">✔Classification</a></td><td align="center">2020/10/21</td></tr><tr><td align="center">3</td><td align="left">CNN</td><td align="left">待完成</td><td align="center"></td></tr></tbody></table><h1 id="P4-Basic-Concept"><a href="#P4-Basic-Concept" class="headerlink" title="P4 Basic Concept"></a>P4 Basic Concept</h1><p><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Bias and Variance如何区分理解？</a></p><p><a href="https://segmentfault.com/a/1190000016447144">偏差(Bias)和方差(Variance)——机器学习中的模型选择</a></p><h1 id="P5Gradien-Descent1"><a href="#P5Gradien-Descent1" class="headerlink" title="P5Gradien Descent1"></a>P5Gradien Descent1</h1><h2 id="Tips1：调整学习速率——自适应学习率"><a href="#Tips1：调整学习速率——自适应学习率" class="headerlink" title="Tips1：调整学习速率——自适应学习率"></a>Tips1：调整学习速率——自适应学习率</h2><p>简单而言：随着次数的增加，通过一些因子来减少学习率</p><p>通常来讲，初始点距离最低点会较远，这时我们想要使用较大的学习率；但是更新几次参数之后，就会距离最低点比较近了，如果学习率依然比较大就容易错过最低点，此时就应该通过减小学习率。</p><p>主要介绍了Adagrad算法，但是当距离最低点很近的时候，会存在学习率过慢的问题；</p><h2 id="Tips2：随机梯度下降算法"><a href="#Tips2：随机梯度下降算法" class="headerlink" title="Tips2：随机梯度下降算法"></a>Tips2：随机梯度下降算法</h2><p>常规的梯度下降算法是走一步要处理所有的例子；而随机算法是每走一步就更新一次梯度；</p><h2 id="Tips3：特征缩放"><a href="#Tips3：特征缩放" class="headerlink" title="Tips3：特征缩放"></a>Tips3：特征缩放</h2><p>把特征的规模缩放成相同的或者差不多的，不要差别太大；</p><h1 id="P10-Classification"><a href="#P10-Classification" class="headerlink" title="P10 Classification"></a>P10 Classification</h1><h3 id="分类模型（二分类）"><a href="#分类模型（二分类）" class="headerlink" title="分类模型（二分类）"></a>分类模型（二分类）</h3><p>在function一个函数g（x），如果大于0，就认为是类别1，否则认为类别2。</p><p>损失函数定义可以是，L（f）在训练集预测错误的次数，当然希望错误次数越小越好；</p><h1 id="P11-Logistic-Regression"><a href="#P11-Logistic-Regression" class="headerlink" title="P11 Logistic Regression"></a>P11 Logistic Regression</h1><h1 id="P12-Brief-Introduction-of-Deep-Learning"><a href="#P12-Brief-Introduction-of-Deep-Learning" class="headerlink" title="P12:Brief Introduction of Deep Learning"></a>P12:Brief Introduction of Deep Learning</h1><p><strong>注意，看完P12可以直接跳过去看P15， 然后再看Ｐ13.</strong></p><h2 id="Three-Steps-for-Deep-Learning"><a href="#Three-Steps-for-Deep-Learning" class="headerlink" title="Three Steps for Deep Learning"></a>Three Steps for Deep Learning</h2><h3 id="Step1-Neural-Network"><a href="#Step1-Neural-Network" class="headerlink" title="Step1:Neural Network"></a>Step1:Neural Network</h3><p>神经网络可以有多种不同的连接方式，这样就会产生不同的结构，期间会用到不同的逻辑回归函数，每个逻辑回归函数都有自己的权重和偏差（就是参数)，而神经元的连接方式是可以手动设计的。</p><h4 id="Fully-Connect-Feedforward-Network完全连接前馈神经网络"><a href="#Fully-Connect-Feedforward-Network完全连接前馈神经网络" class="headerlink" title="Fully Connect Feedforward Network完全连接前馈神经网络"></a>Fully Connect Feedforward Network完全连接前馈神经网络</h4><p>信号输入之后，流动方向是单向的，从前一层流向后一层。</p><p>全连接：相连的两层之间两两都由连接；</p><p>前馈：传递方向是从后向前；</p><p>输入层：一层；隐藏层：N层；输出层：一层；</p><h4 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算"></a>矩阵计算</h4><p>为了降低运算量，我们通过矩阵进行运算，每一层之间都运算一层，并层层嵌套，就相当于是内层函数的结果是外层函数的变量，嵌套多层；</p><h3 id="Step2-Goodness-of-Function"><a href="#Step2-Goodness-of-Function" class="headerlink" title="Step2:Goodness of Function"></a>Step2:Goodness of Function</h3><p>判断一个模型的好坏，和之前在逻辑回归一样，采用cross entropy交叉熵来计算，然后调整参数，让交叉熵越小越好</p><h3 id="Step3：选择最优函数"><a href="#Step3：选择最优函数" class="headerlink" title="Step3：选择最优函数"></a>Step3：选择最优函数</h3><p>通过梯度下降的方法找到最优函数和最好的一组参数，可以参考之前的视频。</p><h1 id="P13：Backpropagation反向传播"><a href="#P13：Backpropagation反向传播" class="headerlink" title="P13：Backpropagation反向传播"></a>P13：Backpropagation反向传播</h1><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>用来求导的，可以回顾高等数学的微积分，不难</p><h2 id="Forward-Pass-与-Backward-Pass"><a href="#Forward-Pass-与-Backward-Pass" class="headerlink" title="Forward Pass 与 Backward Pass"></a>Forward Pass 与 Backward Pass</h2><p>ForwardPass就是正向求导，BackwardPass就是逆向求导，看着公式推一下就可以了。</p><h1 id="P14Tips-for-Training-DNN"><a href="#P14Tips-for-Training-DNN" class="headerlink" title="P14Tips for Training DNN"></a>P14Tips for Training DNN</h1><h1 id="P15Why-Deep"><a href="#P15Why-Deep" class="headerlink" title="P15Why Deep?"></a>P15Why Deep?</h1><p>从深度-&gt;程序的模块化，可以互相调用相同的模块，把复杂的问题变得简单，随着层次的增加，层次的功能变得更加复杂。</p><h1 id="P17CNN"><a href="#P17CNN" class="headerlink" title="P17CNN"></a>P17CNN</h1><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://easyai.tech/ai-definition/cnn/">https://easyai.tech/ai-definition/cnn/</a></p><p><a href="https://www.cnblogs.com/kongweisi/p/10987870.html">https://www.cnblogs.com/kongweisi/p/10987870.html</a></p><p>CNN解决的问题：第一个是将图像维度降低之后再处理，可以简化处理过程，降维并不会影响结果；第二个是保留图像的特征；</p><h2 id="Input-Layer"><a href="#Input-Layer" class="headerlink" title="Input Layer"></a>Input Layer</h2><p>输入层需要对数据进行预处理操作，常见的是：去均值、归一化、PCA/白化。</p><h2 id="Convolution-Max-Pooling-can-repeat-many-times"><a href="#Convolution-Max-Pooling-can-repeat-many-times" class="headerlink" title="Convolution+Max Pooling(can repeat many times)"></a>Convolution+Max Pooling(can repeat many times)</h2><p>卷积层：通过卷积核来提取原始图像的特征</p><p>池化层：有效降低数据的维度，减少运算量，避免过度拟合</p><h3 id="Convolution（卷积层）"><a href="#Convolution（卷积层）" class="headerlink" title="Convolution（卷积层）"></a>Convolution（卷积层）</h3><p>图像中不同数据窗口的数据和卷积核（Filter)作内积的操作叫做卷积，本质是提取图像不同频段的特征。</p><p>处理黑白照片，就是一个matrix与卷积核求内积；处理彩色照片（相当于是RGB三种颜色叠加的图片，相当于是三个通道Chanel），就是三个Matrix与卷积核求内积，其中卷积核也是三层；</p><img src="/2020/10/15/ML-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E6%9D%8E%E5%AE%8F%E6%AF%85%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/p17-2.gif" class><p>将输入矩阵与卷积核进行内积运算，得到一个新的矩阵。卷积核本质上是一个小规模的矩阵，相当于一个小矩阵在大矩阵上进行移动，移动的步长可以设定，最后得到一个新的矩阵。这样做既可以有效提取特征，还可以解决参数数量太大的问题。</p><p>每一个卷积核相当于一个神经元，可以共享同一个卷积核，称为权值共享。</p><p>步长：卷积核在大矩阵上移动的距离，一般设置为1；</p><p>通道：如果原始数据有多层，那么卷积核也要有相同的层次，例如RGB图片，就是由三层不同颜色组成，</p><h3 id="Pooling（池化层）"><a href="#Pooling（池化层）" class="headerlink" title="Pooling（池化层）"></a>Pooling（池化层）</h3><p>作用是压缩数据，减少参数的数目，其方法很多，一般采用Max Pooling，最大池化，只取最大值。</p><p>注意：卷积层和池化层可以多次重复出现，其作用都是一样的，只是卷积核可以根据需要进行调整。</p><h2 id="Fully-Connected-Feedforward-network"><a href="#Fully-Connected-Feedforward-network" class="headerlink" title="Fully Connected Feedforward network"></a>Fully Connected Feedforward network</h2><p>把多维数据进行Flatten，得到一维数据，进行全连接；</p><p>CNN卷积过程：<a href="https://blog.csdn.net/aaa958099161/article/details/90346899">https://blog.csdn.net/aaa958099161/article/details/90346899</a></p><h1 id="P18-19"><a href="#P18-19" class="headerlink" title="P18-19"></a>P18-19</h1>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Hexo搭建博客并部署到GitHub</title>
      <link href="2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/"/>
      <url>2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>跟着网上的教程摸索了一天时间，终于搭建成功，就赶紧写下这篇博客作为记录。按照网上的教程往往是无法一遍完成的，因为对于新手而言很多配置都没有做，但是教程却忽略了这一点，我会在这篇博客中尽可能详尽说明。</p><h1 id="第一、下载并安装Git和Node-js"><a href="#第一、下载并安装Git和Node-js" class="headerlink" title="第一、下载并安装Git和Node.js"></a>第一、下载并安装Git和Node.js</h1><h2 id="（一）下载安装Git"><a href="#（一）下载安装Git" class="headerlink" title="（一）下载安装Git"></a>（一）下载安装Git</h2><ol><li>下载地址<a href="https://gitforwindows.org/">https://gitforwindows.org/</a></li><li>安装过程</li></ol><p>全程默认安装即可，没有太多需要注意的点。</p><h2 id="（二）下载并安装Node-js"><a href="#（二）下载并安装Node-js" class="headerlink" title="（二）下载并安装Node.js"></a>（二）下载并安装Node.js</h2><ol><li><p>下载地址<a href="https://nodejs.org/en/">https://nodejs.org/en/</a></p><p>有两个版本，分别为LTS和Current版本，下载前者即可。</p><p>安装过程</p></li><li><p>全程默认安装即可，没有太多需要注意的点。</p></li><li><p>对于上述两个步骤，如何判断是否安装成功？</p><p>在安装完成后，打开windows的命令行（Win+R并输入cmd打开），依次输入</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line">git --version</span><br></pre></td></tr></table></figure><p>如果可得到对应的版本号，说明安装正确，如下：</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/1.png" class title="This is an test image"><h1 id="第二、安装Hexo"><a href="#第二、安装Hexo" class="headerlink" title="第二、安装Hexo"></a>第二、安装Hexo</h1><p>Hexo是一个快速、简洁且高效的技术博客框架</p><ol><li><p>通过Git Bash安装Hexo，在任意位置右键鼠标，然后选择“Git Bash”如下图所示</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/2.png" class title="This is an test image"></li><li><p>输入命令npm install -g hexo</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/3.png" class title="This is an test image"></li></ol><h1 id="第三、初始化Hexo"><a href="#第三、初始化Hexo" class="headerlink" title="第三、初始化Hexo"></a>第三、初始化Hexo</h1><p>Hexo的一些基本命令，可参照<a href="https://segmentfault.com/a/1190000002632530">https://segmentfault.com/a/1190000002632530</a></p><ol><li><p>建立个文件夹来存放网站的各种元素</p><p>创建文件夹有两种方式，一种是手动创建，直接右键，然后命名即可（如Blog）；另一种是通过Git创建，在你想要创建的目录下，右键选择“Git Bash”，然后输入（<strong>切记后面所有操作都需要在Blog文件夹右键选择GitBash</strong>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd Blog</span><br></pre></td></tr></table></figure></li><li><p>初始化Hexo</p><p>在Blog文件夹下右键Git　Bash，并输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>之后在Blog文件夹下面会出现如下文件</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/4.png" class title="This is an test image"></li><li><p>修改部分基础配置</p><p>配置文件为“_config.yml”，以记事本方式打开，然后可以针对进行修改一些最基本的配置，只需要修改一下标题，先测试一下即可，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Hexo Configuration</span><br><span class="line"></span><br><span class="line">title: 标题</span><br><span class="line">subtitle: 副标题</span><br><span class="line">description: 描述</span><br><span class="line">keywords: 关键词</span><br><span class="line">author: 自己的名字</span><br><span class="line">language: en</span><br><span class="line">timezone: &#39;&#39;</span><br></pre></td></tr></table></figure><p>注意：一定要注意冒号后面有一个空格，千万不能少</p></li><li><p>在本地浏览博客</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>输入以上两个命令之后，直接复制 <a href="http://localhost:4000到浏览器中，即可打开你的博客，如下（主题不同）：">http://localhost:4000到浏览器中，即可打开你的博客，如下（主题不同）：</a></p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/5.png" class title="This is an test image"></li></ol><p>至此网站已经搭建完成，但是暂时只能在本地浏览，下面会讲到如何部署到GitHub上。</p><h1 id="第四、在Hexo发布文章、更改主题"><a href="#第四、在Hexo发布文章、更改主题" class="headerlink" title="第四、在Hexo发布文章、更改主题"></a>第四、在Hexo发布文章、更改主题</h1><h2 id="1、发布文章"><a href="#1、发布文章" class="headerlink" title="1、发布文章"></a>1、发布文章</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;我的第一篇博客&quot;</span><br></pre></td></tr></table></figure><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/6.png" class title="This is an test image"><p>在~/Blog/source/_posts文件加下就会出现一个文件，这个文件时markdown格式，需要单独下载Typora<a href="https://typora.io/">https://typora.io/</a> 来编辑博客，这个软件直接下载安装即可。</p><p>启动本地服务器可以进行预览</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g#生成页面</span><br><span class="line">hexo s#启动预览</span><br></pre></td></tr></table></figure><h2 id="2、更改网站的主题"><a href="#2、更改网站的主题" class="headerlink" title="2、更改网站的主题"></a>2、更改网站的主题</h2><p>Hexo官网给我们提供了很多主题，可以直接调用，参照：<a href="https://hexo.io/themes/">https://hexo.io/themes/</a></p><p>我们以Hiero这个主题为例，进行讲解</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/7.png" class title="This is an test image"><p>点击红色方框的蓝色字体，即可进入Github主页，然后下载压缩包</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/8.png" class title="This is an test image"><p>解压之后把文件复制到~/Blog/themes文件夹之下</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/9.png" class title="This is an test image"><p>修改_config.yml文件（上面讲到了），把theme改为红色方框选中的文件夹名称即可。</p><p>然后输入命令，就可以看到新的主题了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean#清楚缓存</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><h1 id="第五、把Hexo部署到Github"><a href="#第五、把Hexo部署到Github" class="headerlink" title="第五、把Hexo部署到Github"></a>第五、把Hexo部署到Github</h1><p>部署到GitHub是为了可以让大家访问你的博客，而不仅仅是本地浏览</p><ol><li><p>注册并登录GitHub；</p></li><li><p>新建一个仓库，仓库名称为 username.github.io，其中username就是你注册的用户名</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/10.png" class title="This is an test image"></li><li><p>设置用户名和邮箱</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;GitHub的用户名&quot;</span><br><span class="line">git config --global user.email &quot;GitHub的邮箱&quot;</span><br></pre></td></tr></table></figure></li><li><p>创建SSH密匙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;GitHub的邮箱&quot;</span><br></pre></td></tr></table></figure><p>此时，你的C盘会生成一个.ssh文件，用记事本打开该文件夹下的id_rsa.pub文件，并复制其中的内容</p></li><li><p>在GitHub中New SSH Key</p><p>点击头像&gt;setings&gt;SSH and GPG keys，把刚才复制的内容粘贴到红色方框中即可，Title可写可不写</p><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/11.png" class title="This is an test image"></li><li><p>验证连接是否成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/12.png" class title="This is an test image"></li><li><p>部署Hexo到GitHub Pages</p><ul><li><p>安装hexo-deployer-git</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li><li><p>修改_config.yml文件，在最下面的Deployment部分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: 这个地址来自GitHub中我们新建的仓库处，看下图</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><img src="/2020/10/10/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/13.png" class title="This is an test image"></li><li><p>最后重新运行一次即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>此时就可以通过Github的域名https://用户名.github.io来访问</p></li></ul></li></ol><h1 id="后记：我遇到的问题及解决方案"><a href="#后记：我遇到的问题及解决方案" class="headerlink" title="后记：我遇到的问题及解决方案"></a>后记：我遇到的问题及解决方案</h1><h2 id="1、Hexo与GitHub网页主题不一致"><a href="#1、Hexo与GitHub网页主题不一致" class="headerlink" title="1、Hexo与GitHub网页主题不一致"></a>1、Hexo与GitHub网页主题不一致</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#39;首先清除缓存，然后生成待发布的文件，最后发布&#39;</span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><h2 id="2、在markdown插入的图片无法在网页显示"><a href="#2、在markdown插入的图片无法在网页显示" class="headerlink" title="2、在markdown插入的图片无法在网页显示"></a>2、在markdown插入的图片无法在网页显示</h2><p><a href="https://alexcld.com/2020/08/14/%E8%A7%A3%E5%86%B3Hexo%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/">https://alexcld.com/2020/08/14/%E8%A7%A3%E5%86%B3Hexo%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/</a></p><p>已测试可以使用</p><h2 id="3、更新主题并配置主题"><a href="#3、更新主题并配置主题" class="headerlink" title="3、更新主题并配置主题"></a>3、更新主题并配置主题</h2><p>最复杂的是网站主题的更新及一些配置，首先需要找到一个自己喜欢的主题；其次根据作者的操作文档进行更新一些配置，更新过程中可一边更新一边部署，这样可以防止出bug；</p><h2 id="4、fatal-Could-not-read-from-remote-repository-Please-make-sure-you-have-the-correct-access-rights-and-the-repository-exists"><a href="#4、fatal-Could-not-read-from-remote-repository-Please-make-sure-you-have-the-correct-access-rights-and-the-repository-exists" class="headerlink" title="4、fatal: Could not read from remote repository.Please make sure you have the correct access rights and the repository exists."></a>4、fatal: Could not read from remote repository.Please make sure you have the correct access rights and the repository exists.</h2><p>刚看到之后也很郁闷，经过网上一番搜索之后，只需要把ssh这个步骤重复一次即可，即第五步骤的3-6这个步骤重复，在重复之前需要删除ssh文件夹中的“known_host”文件。</p><p>不过有时候可能是GitHub上传到服务器比较慢，多等一会就可以部署。</p><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p><a href="https://zhuanlan.zhihu.com/p/60578464">https://zhuanlan.zhihu.com/p/60578464</a></p><p><a href="https://www.jianshu.com/p/189fd945f38f">https://www.jianshu.com/p/189fd945f38f</a></p><p><a href="https://demo.jerryc.me/">https://demo.jerryc.me/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
